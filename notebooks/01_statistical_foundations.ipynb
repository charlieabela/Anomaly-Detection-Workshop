{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42f6561",
   "metadata": {},
   "source": [
    "# Notebookâ€¯1 â€“ Statistical Foundations of Anomaly Detection  \n",
    "*Generated 2025-05-13*\n",
    "\n",
    "This guided notebook complements **Slidesâ€¯1â€‘20** of the deck, covering the statistical core of anomaly detection.\n",
    "\n",
    "> Work in pairs: swap driver/navigator roles at each ğŸ›‘ checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca726a8",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ What is Anomaly Detection?\n",
    "\n",
    "*Anomaly* â‰ˆ observation that **deviates** so much from other observations that it arouses suspicions it was generated by a **different mechanism**.\n",
    "\n",
    "### Realâ€‘world examples\n",
    "| Domain | Anomaly instance |\n",
    "|--------|------------------|\n",
    "| Finance | Sudden $9â€¯999 charge on dormant card |\n",
    "| Health | Irregular heartâ€‘rate spike in ECG |\n",
    "| Cybersecurity | 50Â login attempts from a new IP in 10â€¯s |\n",
    "\n",
    "### Taxonomy of anomalies\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Point** | Single data point abnormal in feature space | Fraudulent transaction amount |\n",
    "| **Contextual** | Normal globally, abnormal in local context | 18â€¯Â°C reading in a boiler thatâ€™s usually 95â€¯Â°C |\n",
    "| **Collective** | Group of points abnormal as a sequence | SlowÂ exfiltration pattern across many small packets |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¥ Load the benchmark dataset\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df['Class'].value_counts(normalize=True).rename({0:'Legit',1:'Fraud'}))\n",
    "\n",
    "# Separate features / label and scale\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec68a4",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Univariate Robust Zâ€‘Score\n",
    "\n",
    "For each feature $x$:\n",
    "\n",
    "$$ z = \\frac{|x - \\tilde{x}|}{\\text{MAD}} $$\n",
    "\n",
    "*Where*  \n",
    "\\( \\tilde{x} \\)Â =Â medianâ€ƒâ€ƒMADÂ =Â Median Absolute Deviation.\n",
    "\n",
    "**Workflow**\n",
    "\n",
    "1. Compute *perâ€‘feature* robust $z$.  \n",
    "2. Aggregate per row (sum).  \n",
    "3. Flag rows above a percentile threshold (e.g., 99.9â€¯%).  \n",
    "\n",
    "**Strengths**\n",
    "\n",
    "* Simple, fast, interpretable.  \n",
    "* Robust medianÂ &Â MAD resist skewed data.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Ignores feature correlation.  \n",
    "* Struggles with contextual/collective anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f46e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import median_abs_deviation\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "med = X.median()\n",
    "mad = X.apply(median_abs_deviation).replace(0, 1e-6)\n",
    "z_scores = ((X - med) / mad).abs()\n",
    "df['z_sum'] = z_scores.sum(axis=1)\n",
    "\n",
    "# Threshold top 0.1%\n",
    "thresh = np.quantile(df['z_sum'], 0.999)\n",
    "pred_z = (df['z_sum'] > thresh).astype(int)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y, df['z_sum'])\n",
    "pr_auc_z = auc(recall, precision)\n",
    "print(f\"Robust Zâ€‘score PRâ€‘AUC: {pr_auc_z:.4f}\")\n",
    "\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Robust Zâ€‘score â€“ PR curve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934e4ae",
   "metadata": {},
   "source": [
    "### ğŸ“ Interpreting the Robustâ€¯Zâ€‘score output\n",
    "\n",
    "* **PRâ€‘AUC value** printed in the previous cell quantifies discrimination under extreme class imbalance.  \n",
    "  *A PRâ€‘AUC close to 1.0 â‡’ almost perfect; 0.5 â‡’ random; <0.5 â‡’ worse than random.*\n",
    "\n",
    "* **Curve shape**  \n",
    "  *Â Steep initial rise* â†’ method captures the *most obvious* anomalies with very high precision.  \n",
    "  *Â Sharp drop* â†’ many false positives once threshold moves past the extreme tail.\n",
    "\n",
    "* **Threshold insight**  \n",
    "  The 99.9â€‘th percentile threshold flagged roughly 0.1â€¯% of rows.  \n",
    "  If early precision is high but recall low, lowering the threshold (e.g. 99.7â€¯%) would **recover more anomalies** at the cost of extra investigations.\n",
    "\n",
    "**Takeâ€‘away**: Robust Zâ€‘score is a good *firstâ€‘pass* detector when anomalies are extreme univariate deviations, but correlationâ€‘based anomalies will slip through.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b196a",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Multivariate Mahalanobis Distance\n",
    "\n",
    "$$ D_M(x) = \\sqrt{(x-\\mu)^T\\,\\Sigma^{-1}(x-\\mu)} $$\n",
    "\n",
    "*Where*  \n",
    "\\( \\mu \\)Â =Â mean vectorâ€ƒâ€ƒ\\( \\Sigma \\)= covariance matrix.\n",
    "\n",
    "> Intuition: distance in **whitened** space accounts for feature variance *and* correlation.\n",
    "\n",
    "**Practical tips**\n",
    "\n",
    "* Need **nÂ â‰«Â p** to get stable covariance.  \n",
    "* Use a **robust covariance estimator** (e.g., Ledoitâ€‘Wolf) when outliers present.\n",
    "\n",
    "**Strengths**\n",
    "\n",
    "* Captures interâ€‘feature correlation.  \n",
    "* Parametric; threshold via chiâ€‘square distribution.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Sensitive to illâ€‘conditioned covariance.  \n",
    "* Assumes elliptical clusters (multivariate normal).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Fit robust covariance on scaled data\n",
    "cov_est = LedoitWolf().fit(X_scaled)\n",
    "inv_cov = cov_est.get_precision()\n",
    "mean_vec = X_scaled.mean(axis=0)\n",
    "\n",
    "mahal_d = np.array([mahalanobis(row, mean_vec, inv_cov) for row in X_scaled])\n",
    "df['mahal'] = mahal_d\n",
    "\n",
    "# Threshold top 0.1%\n",
    "thresh_m = np.quantile(df['mahal'], 0.999)\n",
    "pred_m = (df['mahal'] > thresh_m).astype(int)\n",
    "\n",
    "precision_m, recall_m, _ = precision_recall_curve(y, df['mahal'])\n",
    "pr_auc_m = auc(recall_m, precision_m)\n",
    "print(f\"Mahalanobis PRâ€‘AUC: {pr_auc_m:.4f}\")\n",
    "\n",
    "plt.plot(recall_m, precision_m)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Mahalanobis â€“ PR curve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8162047",
   "metadata": {},
   "source": [
    "### ğŸ“ Interpreting the Mahalanobis output\n",
    "\n",
    "* **PRâ€‘AUC & curve**  \n",
    "  Compare the reported AUC to the Zâ€‘scoreâ€™s.  A higher AUC means modelling covariance picked up multivariate outliers the univariate rule missed.\n",
    "\n",
    "* **Curve shape**  \n",
    "  Flatter precision early on?  Indicates Mahalanobis sometimes classifies *borderline* points as anomalousâ€”common when covariance is estimated from mixed data.\n",
    "\n",
    "* **Data prerequisites**  \n",
    "  Stable Mahalanobis performance depends on **nÂ â‰«Â p** and roughly elliptical normal class.  Examine if low AUC traces back to a poorly conditioned covariance matrix.\n",
    "\n",
    "**Implication**: Use Mahalanobis when correlation structure matters (e.g. velocity vs distance), but validate that covariance estimation is robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f7f8f3",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Comparative Analysis\n",
    "\n",
    "Below we overlay the Precisionâ€‘Recall curves of both methods and summarise AUCs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f\"Zâ€‘score (AUC={pr_auc_z:.3f})\")\n",
    "plt.plot(recall_m, precision_m, label=f\"Mahalanobis (AUC={pr_auc_m:.3f})\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR Comparison\")\n",
    "plt.legend()\n",
    "\n",
    "# Bar chart for quick comparison\n",
    "plt.figure()\n",
    "plt.bar([\"Zâ€‘score\",\"Mahalanobis\"], [pr_auc_z, pr_auc_m])\n",
    "plt.ylabel(\"PRâ€‘AUC\"); plt.title(\"AUC comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a4824",
   "metadata": {},
   "source": [
    "### ğŸ“Š Comparative plot interpretation\n",
    "\n",
    "* **PR overlay** â€“ Whichever curve sits *highest across most recall values* offers the best precisionâ€‘recall tradeâ€‘off overall.\n",
    "\n",
    "* **Bar chart**  \n",
    "  Quick numerical snapshot:  \n",
    "  *Î”Â AUCÂ â‰¥Â 0.05* is usually **practically significant**; smaller gaps may not justify a more complex method.\n",
    "\n",
    "* **Decision guideline**\n",
    "\n",
    "| Scenario | Preferred method |\n",
    "|----------|------------------|\n",
    "| Need **explainable rule** & anomalies are *extreme spikes* | Robust Zâ€‘score |\n",
    "| Anomalies hide in *correlated feature space* | Mahalanobis |\n",
    "| Realâ€‘time streaming, minimal compute | Zâ€‘score (constantâ€‘time update) |\n",
    "\n",
    "Always align the choice with domain cost of *false negatives* vs *false positives*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c83bc2",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Knowledge Check â€“ Discuss & Answer\n",
    "\n",
    "1. **Conceptual** â€“ Give one realâ€‘world example of a *contextual* anomaly and explain why a univariate Zâ€‘score might miss it.  \n",
    "2. **Technical** â€“ If the MAD of a feature is *zero*, what adjustment should you make before computing the Zâ€‘score?  \n",
    "3. **Mahalanobis** â€“ Why does multicollinearity in features risk inflating Mahalanobis distances?  \n",
    "4. **Thresholding** â€“ Describe one dataâ€‘driven way (other than a fixed percentile) to choose an anomaly threshold.  \n",
    "5. **Critical thinking** â€“ In our experiment, which method achieved higher PRâ€‘AUC and what property of the dataset might explain this?\n",
    "\n",
    "> *Write your answers in the provided reflection sheet or discuss with your partner.*\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
