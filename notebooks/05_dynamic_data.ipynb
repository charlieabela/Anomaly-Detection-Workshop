{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection in Dynamic (Time Series) Data\n",
        "> A comprehensive exploration of methods for detecting anomalies in time-varying data\n",
        "\n",
        "In this notebook, we'll explore various methods for detecting anomalies in time series data, from traditional statistical approaches to advanced deep learning techniques using LSTMs and autoencoders.\n",
        "\n",
        "What you'll learn:\n",
        "* Types of time series anomalies and their characteristics\n",
        "* Statistical anomaly detection methods (Moving Window, ARIMA)\n",
        "* Neural Network approaches (Feedforward, RNN)\n",
        "* LSTM architecture with detailed gate mechanisms\n",
        "* Autoencoders for unsupervised anomaly detection\n",
        "\n",
        "By the end, you'll be able to implement and compare these approaches on real-world data.\n"
      ],
      "metadata": {
        "id": "0vRVN1KerHWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install numpy pandas matplotlib scikit-learn tensorflow statsmodels seaborn plotly"
      ],
      "metadata": {
        "id": "LwYaHwamrN4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Understanding Anomalies in Time Series\n",
        "\n",
        "Time series anomalies come in three main types:\n",
        "\n",
        "1. **Point Anomalies**: Individual data points that significantly deviate from the normal behavior pattern\n",
        "   - Example: Sudden spike in system memory usage\n",
        "\n",
        "2. **Contextual Anomalies**: Data points that are anomalous in a specific context but not otherwise\n",
        "   - Example: 70°F temperature in winter vs. summer\n",
        "\n",
        "3. **Collective Anomalies**: Sequences of data points that together form an anomalous pattern\n",
        "   - Example: ECG readings showing arrhythmia across multiple measurements\n",
        "\n",
        "Let's start by generating synthetic data with different types of anomalies:\n"
      ],
      "metadata": {
        "id": "U7_MdyVhrRP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Function to generate synthetic time series with different anomaly types\n",
        "def generate_synthetic_data(n_samples=1000):\n",
        "    # Generate timestamp index\n",
        "    start_date = datetime(2023, 1, 1)\n",
        "    timestamps = [start_date + timedelta(hours=i) for i in range(n_samples)]\n",
        "\n",
        "    # Generate base signal: trend + seasonality + noise\n",
        "    t = np.arange(n_samples)\n",
        "    trend = 0.01 * t\n",
        "    daily_seasonality = 10 * np.sin(2 * np.pi * t / 24)  # 24-hour cycle\n",
        "    weekly_seasonality = 5 * np.sin(2 * np.pi * t / (24 * 7))  # Weekly cycle\n",
        "    noise = np.random.normal(0, 1, n_samples)\n",
        "\n",
        "    # Combine components\n",
        "    signal = trend + daily_seasonality + weekly_seasonality + noise\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'timestamp': timestamps,\n",
        "        'value': signal,\n",
        "        'anomaly': np.zeros(n_samples, dtype=bool)  # Initialize anomaly label\n",
        "    })\n",
        "\n",
        "    # Add point anomalies (sudden spikes)\n",
        "    point_anomaly_indices = [100, 300, 700]\n",
        "    for idx in point_anomaly_indices:\n",
        "        df.loc[idx, 'value'] += 25  # Add large spike\n",
        "        df.loc[idx, 'anomaly'] = True\n",
        "\n",
        "    # Add contextual anomaly (unusual pattern during specific time)\n",
        "    # Night hours with high values (unusual)\n",
        "    contextual_start = 400\n",
        "    for i in range(5):\n",
        "        # Assuming night hours should have lower values\n",
        "        idx = contextual_start + i\n",
        "        hour = df.loc[idx, 'timestamp'].hour\n",
        "        if hour >= 0 and hour <= 6:  # Night hours\n",
        "            df.loc[idx, 'value'] += 15  # Unusual high value at night\n",
        "            df.loc[idx, 'anomaly'] = True\n",
        "\n",
        "    # Add collective anomaly (unusual sequence pattern)\n",
        "    collective_start = 800\n",
        "    for i in range(20):\n",
        "        idx = collective_start + i\n",
        "        # Damped oscillation pattern (different from normal seasonality)\n",
        "        df.loc[idx, 'value'] += 10 * np.sin(i) * np.exp(-i/10)\n",
        "        df.loc[idx, 'anomaly'] = True\n",
        "\n",
        "    return df\n",
        "\n",
        "# Generate data\n",
        "ts_data = generate_synthetic_data()\n",
        "\n",
        "# Visualize the time series with anomalies\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data['timestamp'], ts_data['value'], color='blue', label='Normal')\n",
        "plt.scatter(ts_data[ts_data['anomaly']]['timestamp'],\n",
        "            ts_data[ts_data['anomaly']]['value'],\n",
        "            color='red', label='Anomaly')\n",
        "plt.title('Synthetic Time Series with Different Types of Anomalies')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Display summary statistics\n",
        "print(f\"Total data points: {len(ts_data)}\")\n",
        "print(f\"Number of anomalies: {ts_data['anomaly'].sum()}\")\n",
        "print(f\"Anomaly percentage: {100 * ts_data['anomaly'].sum() / len(ts_data):.2f}%\")\n"
      ],
      "metadata": {
        "id": "LqsBP_LFrT_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Statistical Methods for Anomaly Detection\n",
        "\n",
        "Statistical methods rely on mathematical principles to identify data points that significantly deviate from expected patterns. Let's explore two common approaches:\n",
        "\n",
        "### 2.1 Moving Window Statistics\n",
        "This approach uses a sliding window to calculate local statistics (mean, standard deviation) and flags points that deviate beyond a certain threshold.\n"
      ],
      "metadata": {
        "id": "C2f8mJtHrYWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_window_detector(series, window_size=24, n_sigmas=3):\n",
        "    \"\"\"\n",
        "    Detect anomalies using a moving window approach.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    series: pandas.Series\n",
        "        Time series data\n",
        "    window_size: int\n",
        "        Size of the moving window\n",
        "    n_sigmas: float\n",
        "        Number of standard deviations to use for thresholding\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.Series\n",
        "        Boolean series with True for detected anomalies\n",
        "    \"\"\"\n",
        "    # Initialize results array\n",
        "    anomalies = np.zeros(len(series), dtype=bool)\n",
        "\n",
        "    # The first window_size points cannot be evaluated (not enough history)\n",
        "    anomalies[:window_size] = False\n",
        "\n",
        "    # Calculate rolling statistics\n",
        "    rolling_mean = series.rolling(window=window_size).mean()\n",
        "    rolling_std = series.rolling(window=window_size).std()\n",
        "\n",
        "    # Define upper and lower thresholds\n",
        "    upper_threshold = rolling_mean + n_sigmas * rolling_std\n",
        "    lower_threshold = rolling_mean - n_sigmas * rolling_std\n",
        "\n",
        "    # Flag anomalies\n",
        "    anomalies[window_size:] = ((series[window_size:] > upper_threshold[window_size:]) |\n",
        "                              (series[window_size:] < lower_threshold[window_size:]))\n",
        "\n",
        "    return pd.Series(anomalies, index=series.index)\n",
        "\n",
        "# Apply moving window detector\n",
        "window_size = 24  # One day\n",
        "n_sigmas = 3      # Three standard deviations\n",
        "mw_anomalies = moving_window_detector(ts_data['value'], window_size, n_sigmas)\n",
        "\n",
        "# Calculate performance metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Function to calculate and display metrics\n",
        "def calculate_metrics(y_true, y_pred, method_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Method: {method_name}\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "# Calculate metrics for moving window method\n",
        "mw_metrics = calculate_metrics(ts_data['anomaly'], mw_anomalies, \"Moving Window\")\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data['timestamp'], ts_data['value'], color='blue', label='Data')\n",
        "plt.scatter(ts_data[ts_data['anomaly']]['timestamp'],\n",
        "            ts_data[ts_data['anomaly']]['value'],\n",
        "            color='red', marker='o', label='True Anomaly')\n",
        "plt.scatter(ts_data[mw_anomalies]['timestamp'],\n",
        "            ts_data[mw_anomalies]['value'],\n",
        "            color='orange', marker='x', label='Moving Window Detection')\n",
        "plt.title('Moving Window Anomaly Detection')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visual explanation of the moving window approach\n",
        "sample_start = 90\n",
        "sample_end = 110\n",
        "sample_data = ts_data.iloc[sample_start:sample_end]\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Data points\n",
        "plt.plot(sample_data['timestamp'], sample_data['value'], 'b-', label='Time Series')\n",
        "plt.scatter(sample_data[sample_data['anomaly']]['timestamp'],\n",
        "            sample_data[sample_data['anomaly']]['value'],\n",
        "            color='red', s=100, label='True Anomaly')\n",
        "\n",
        "# For a specific point, show the window used to calculate statistics\n",
        "point_idx = 100\n",
        "window_data = ts_data.iloc[point_idx-window_size:point_idx]\n",
        "window_mean = window_data['value'].mean()\n",
        "window_std = window_data['value'].std()\n",
        "\n",
        "# Shade the window area\n",
        "plt.axvspan(window_data['timestamp'].iloc[0], window_data['timestamp'].iloc[-1],\n",
        "           color='lightblue', alpha=0.3, label='Window')\n",
        "\n",
        "# Show thresholds\n",
        "plt.axhline(y=window_mean + n_sigmas*window_std, color='red', linestyle='--', alpha=0.7,\n",
        "           label=f'Mean + {n_sigmas}σ')\n",
        "plt.axhline(y=window_mean - n_sigmas*window_std, color='red', linestyle='--', alpha=0.7,\n",
        "           label=f'Mean - {n_sigmas}σ')\n",
        "plt.axhline(y=window_mean, color='green', linestyle='-', alpha=0.7, label='Window Mean')\n",
        "\n",
        "plt.title('Moving Window Approach Visualization')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GXP0PCrdrafX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 ARIMA-based Anomaly Detection\n",
        "\n",
        "AutoRegressive Integrated Moving Average (ARIMA) models are classical time series forecasting models that can be used for anomaly detection. The approach is:\n",
        "\n",
        "1. Fit an ARIMA model to the time series\n",
        "2. Use the model to forecast values\n",
        "3. Compare actual values with forecasted values\n",
        "4. Flag significant deviations as anomalies\n",
        "\n",
        "ARIMA models have three components:\n",
        "- **AR(p)**: AutoRegressive term - uses the relationship between an observation and p lagged observations\n",
        "- **I(d)**: Integrated - differencing to make the data stationary  \n",
        "- **MA(q)**: Moving Average term - uses the dependency between an observation and a residual error from a moving average model applied to q lagged observations\n"
      ],
      "metadata": {
        "id": "TrTAUqNurd5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress convergence warnings\n",
        "\n",
        "def arima_detector(series, order=(1,1,1), n_sigmas=3, train_size=0.7):\n",
        "    \"\"\"\n",
        "    Detect anomalies using an ARIMA model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    series: pandas.Series\n",
        "        Time series data\n",
        "    order: tuple\n",
        "        ARIMA model order (p,d,q)\n",
        "    n_sigmas: float\n",
        "        Number of standard deviations to use for thresholding\n",
        "    train_size: float\n",
        "        Proportion of data to use for training\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.Series\n",
        "        Boolean series with True for detected anomalies\n",
        "    \"\"\"\n",
        "    # Split data into training and testing\n",
        "    train_size_idx = int(len(series) * train_size)\n",
        "    train = series[:train_size_idx]\n",
        "    test = series[train_size_idx:]\n",
        "\n",
        "    # Fit ARIMA model on training data\n",
        "    model = ARIMA(train, order=order)\n",
        "    model_fit = model.fit()\n",
        "\n",
        "    # Make predictions on the entire dataset\n",
        "    predictions = model_fit.predict(start=0, end=len(series)-1)\n",
        "\n",
        "    # Calculate residuals (errors)\n",
        "    residuals = series - predictions\n",
        "\n",
        "    # Calculate threshold based on residual statistics\n",
        "    residual_mean = residuals[:train_size_idx].mean()\n",
        "    residual_std = residuals[:train_size_idx].std()\n",
        "\n",
        "    # Define anomaly thresholds\n",
        "    upper_threshold = residual_mean + n_sigmas * residual_std\n",
        "    lower_threshold = residual_mean - n_sigmas * residual_std\n",
        "\n",
        "    # Flag anomalies\n",
        "    anomalies = (residuals > upper_threshold) | (residuals < lower_threshold)\n",
        "\n",
        "    return anomalies, predictions\n",
        "\n",
        "# Apply ARIMA-based detector\n",
        "arima_order = (2, 1, 2)  # p, d, q\n",
        "arima_anomalies, arima_predictions = arima_detector(ts_data['value'], order=arima_order)\n",
        "\n",
        "# Calculate metrics for ARIMA method\n",
        "arima_metrics = calculate_metrics(ts_data['anomaly'], arima_anomalies, \"ARIMA\")\n",
        "\n",
        "# Visualize ARIMA results\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data['timestamp'], ts_data['value'], color='blue', label='Original Data')\n",
        "plt.plot(ts_data['timestamp'], arima_predictions, color='green', alpha=0.7, label='ARIMA Predictions')\n",
        "plt.scatter(ts_data[ts_data['anomaly']]['timestamp'],\n",
        "            ts_data[ts_data['anomaly']]['value'],\n",
        "            color='red', marker='o', label='True Anomaly')\n",
        "plt.scatter(ts_data[arima_anomalies]['timestamp'],\n",
        "            ts_data[arima_anomalies]['value'],\n",
        "            color='purple', marker='x', label='ARIMA Detection')\n",
        "plt.title('ARIMA-based Anomaly Detection')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualize residuals and thresholds\n",
        "residuals = ts_data['value'] - arima_predictions\n",
        "train_size_idx = int(len(ts_data) * 0.7)\n",
        "residual_mean = residuals[:train_size_idx].mean()\n",
        "residual_std = residuals[:train_size_idx].std()\n",
        "upper_threshold = residual_mean + 3 * residual_std\n",
        "lower_threshold = residual_mean - 3 * residual_std\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data['timestamp'], residuals, color='blue', label='Residuals')\n",
        "plt.axhline(y=upper_threshold, color='red', linestyle='--', label='Upper Threshold')\n",
        "plt.axhline(y=lower_threshold, color='red', linestyle='--', label='Lower Threshold')\n",
        "plt.axhline(y=residual_mean, color='green', linestyle='-', label='Mean')\n",
        "plt.scatter(ts_data[arima_anomalies]['timestamp'],\n",
        "            residuals[arima_anomalies],\n",
        "            color='purple', marker='x', label='Detected Anomalies')\n",
        "plt.title('ARIMA Residuals with Anomaly Thresholds')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Residual')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XPO9wJcBrgg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Neural Network Approaches for Anomaly Detection\n",
        "\n",
        "Neural networks provide powerful tools for anomaly detection due to their ability to learn complex patterns. Let's explore their application to time series data.\n",
        "\n",
        "### 3.1 Feedforward Neural Networks\n",
        "\n",
        "Feedforward Neural Networks (FFNNs) can be adapted for time series by using a sliding window approach to create input-output pairs from sequential data.\n"
      ],
      "metadata": {
        "id": "ukUKZPRXrj3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"\n",
        "    Create input-output pairs from sequential data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data: numpy.ndarray\n",
        "        Input time series data\n",
        "    seq_length: int\n",
        "        Length of input sequence (window size)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X: numpy.ndarray\n",
        "        Input sequences\n",
        "    y: numpy.ndarray\n",
        "        Target values (next point after each sequence)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Preprocess data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(ts_data['value'].values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 24  # Use 24-hour window to capture daily patterns\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "\n",
        "# Split into train/test sets ensuring no anomalies in training\n",
        "train_indices = np.where(ts_data['anomaly'][seq_length:] == False)[0]\n",
        "test_indices = np.arange(len(X))\n",
        "test_indices = np.setdiff1d(test_indices, train_indices)\n",
        "\n",
        "X_train, y_train = X[train_indices], y[train_indices]\n",
        "X_test, y_test = X[test_indices], y[test_indices]\n",
        "\n",
        "# Build feedforward neural network\n",
        "ffnn_model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(seq_length,)),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "ffnn_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    mode='min',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = ffnn_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('FFNN Model Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Use model for anomaly detection\n",
        "def ffnn_detector(model, X, y, threshold_multiplier=3):\n",
        "    \"\"\"\n",
        "    Detect anomalies using prediction errors from a neural network.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model: tensorflow.keras.Model\n",
        "        Trained neural network model\n",
        "    X: numpy.ndarray\n",
        "        Input sequences\n",
        "    y: numpy.ndarray\n",
        "        Actual target values\n",
        "    threshold_multiplier: float\n",
        "        Multiplier for standard deviation to set threshold\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    numpy.ndarray\n",
        "        Boolean array with True for anomalies\n",
        "    \"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Calculate errors\n",
        "    errors = np.abs(y - y_pred.flatten())\n",
        "\n",
        "    # Set threshold based on error statistics\n",
        "    threshold = np.mean(errors) + threshold_multiplier * np.std(errors)\n",
        "\n",
        "    # Detect anomalies\n",
        "    anomalies = errors > threshold\n",
        "\n",
        "    return anomalies, errors, threshold\n",
        "\n",
        "# Apply FFNN detector to entire dataset\n",
        "X_all, y_all = create_sequences(data_scaled, seq_length)\n",
        "ffnn_anomalies, ffnn_errors, ffnn_threshold = ffnn_detector(ffnn_model, X_all, y_all)\n",
        "\n",
        "# Map back to original time points\n",
        "ts_data_ffnn = ts_data.iloc[seq_length:].copy()\n",
        "ts_data_ffnn['ffnn_anomaly'] = ffnn_anomalies\n",
        "ts_data_ffnn['ffnn_error'] = ffnn_errors\n",
        "\n",
        "# Calculate metrics for FFNN method\n",
        "ffnn_metrics = calculate_metrics(ts_data_ffnn['anomaly'], ts_data_ffnn['ffnn_anomaly'], \"Feedforward Neural Network\")\n",
        "\n",
        "# Visualize FFNN results\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data_ffnn['timestamp'], ts_data_ffnn['value'], color='blue', label='Data')\n",
        "plt.scatter(ts_data_ffnn[ts_data_ffnn['anomaly']]['timestamp'],\n",
        "            ts_data_ffnn[ts_data_ffnn['anomaly']]['value'],\n",
        "            color='red', marker='o', label='True Anomaly')\n",
        "plt.scatter(ts_data_ffnn[ts_data_ffnn['ffnn_anomaly']]['timestamp'],\n",
        "            ts_data_ffnn[ts_data_ffnn['ffnn_anomaly']]['value'],\n",
        "            color='green', marker='x', label='FFNN Detection')\n",
        "plt.title('Feedforward Neural Network Anomaly Detection')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualize prediction errors\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data_ffnn['timestamp'], ffnn_errors, color='blue', label='Prediction Error')\n",
        "plt.axhline(y=ffnn_threshold, color='red', linestyle='--', label='Threshold')\n",
        "plt.scatter(ts_data_ffnn[ts_data_ffnn['ffnn_anomaly']]['timestamp'],\n",
        "            ts_data_ffnn[ts_data_ffnn['ffnn_anomaly']]['ffnn_error'],\n",
        "            color='green', marker='x', label='Detected Anomalies')\n",
        "plt.title('FFNN Prediction Errors with Anomaly Threshold')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Absolute Error')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "osq1Ba9Lrjcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Recurrent Neural Networks (RNNs)\n",
        "\n",
        "RNNs are designed specifically for sequential data, with connections that form cycles allowing information to persist. However, basic RNNs suffer from the vanishing gradient problem, which limits their ability to learn long-term dependencies.\n",
        "\n",
        "#### Vanishing Gradient Problem\n",
        "\n",
        "The vanishing gradient problem occurs when gradients are repeatedly multiplied through many time steps during backpropagation. Since these gradients are typically small (< 1), they tend to vanish exponentially, preventing the network from learning long-term dependencies.\n",
        "\n",
        "Mathematically, for a simple RNN, the gradient calculation involves a product chain:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W} \\propto \\prod_{i=1}^{n} \\frac{\\partial h_i}{\\partial h_{i-1}}$$\n",
        "\n",
        "For activation functions like tanh, the derivative is ≤ 0.25 at all points. After n steps, the gradient becomes ≤ 0.25^n, which approaches zero quickly for large n.\n"
      ],
      "metadata": {
        "id": "YYuDcHpErrDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "# Build a simple RNN model\n",
        "rnn_model = Sequential([\n",
        "    SimpleRNN(32, activation='tanh', input_shape=(seq_length, 1), return_sequences=False),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "rnn_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Reshape input for RNN (samples, time steps, features)\n",
        "X_train_rnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_rnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Train the model\n",
        "history_rnn = rnn_model.fit(\n",
        "    X_train_rnn, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Visualize RNN training\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(history_rnn.history['loss'], label='Training Loss')\n",
        "plt.plot(history_rnn.history['val_loss'], label='Validation Loss')\n",
        "plt.title('RNN Model Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Demonstrate vanishing gradient with a custom visualization\n",
        "def visualize_vanishing_gradient():\n",
        "    # Create a simple chain of multiplication to simulate backpropagation\n",
        "    chain_lengths = [5, 10, 20, 50, 100]\n",
        "    gradients = []\n",
        "\n",
        "    for length in chain_lengths:\n",
        "        # Simulating tanh derivative (max value 0.25) multiplied through the chain\n",
        "        gradient = np.power(0.25, length)\n",
        "        gradients.append(gradient)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(len(chain_lengths)), gradients, tick_label=[str(l) for l in chain_lengths])\n",
        "    plt.yscale('log')  # Log scale to better visualize small values\n",
        "    plt.title('Vanishing Gradient Problem Visualization')\n",
        "    plt.xlabel('Sequence Length')\n",
        "    plt.ylabel('Gradient Magnitude (log scale)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value annotations\n",
        "    for i, v in enumerate(gradients):\n",
        "        plt.text(i, v, f'{v:.2e}', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "visualize_vanishing_gradient()\n"
      ],
      "metadata": {
        "id": "QelRZNaorqUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTM networks are specifically designed to address the vanishing gradient problem by introducing a cell state and three specialized gates that control information flow.\n",
        "\n",
        "#### LSTM Architecture\n",
        "\n",
        "1. **Forget Gate**: Controls what information to discard from the cell state\n",
        "   - $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
        "\n",
        "2. **Input Gate**: Controls what new information to add to the cell state\n",
        "   - $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
        "   - $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$ (candidate values)\n",
        "   - Cell state update: $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n",
        "\n",
        "3. **Output Gate**: Controls what information to output\n",
        "   - $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
        "   - Hidden state: $h_t = o_t * \\tanh(C_t)$\n",
        "\n",
        "This architecture allows LSTMs to learn long-term dependencies much more effectively than standard RNNs.\n"
      ],
      "metadata": {
        "id": "4Qrez_LRrwwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# Build LSTM model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(32, activation='tanh', input_shape=(seq_length, 1), return_sequences=False),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history_lstm = lstm_model.fit(\n",
        "    X_train_rnn,  # Reuse the RNN-formatted input\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot LSTM training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(history_lstm.history['loss'], label='Training Loss')\n",
        "plt.plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTM Model Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# LSTM Anomaly Detection\n",
        "X_all_lstm = X_all.reshape(X_all.shape[0], X_all.shape[1], 1)\n",
        "y_pred_lstm = lstm_model.predict(X_all_lstm).flatten()\n",
        "lstm_errors = np.abs(y_all - y_pred_lstm)\n",
        "lstm_threshold = np.mean(lstm_errors) + 3 * np.std(lstm_errors)\n",
        "lstm_anomalies = lstm_errors > lstm_threshold\n",
        "\n",
        "# Map back to original time points\n",
        "ts_data_lstm = ts_data.iloc[seq_length:].copy()\n",
        "ts_data_lstm['lstm_anomaly'] = lstm_anomalies\n",
        "ts_data_lstm['lstm_error'] = lstm_errors\n",
        "\n",
        "# Calculate metrics for LSTM method\n",
        "lstm_metrics = calculate_metrics(ts_data_lstm['anomaly'], ts_data_lstm['lstm_anomaly'], \"LSTM\")\n",
        "\n",
        "# Visualize LSTM results\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data_lstm['timestamp'], ts_data_lstm['value'], color='blue', label='Data')\n",
        "plt.scatter(ts_data_lstm[ts_data_lstm['anomaly']]['timestamp'],\n",
        "            ts_data_lstm[ts_data_lstm['anomaly']]['value'],\n",
        "            color='red', marker='o', label='True Anomaly')\n",
        "plt.scatter(ts_data_lstm[ts_data_lstm['lstm_anomaly']]['timestamp'],\n",
        "            ts_data_lstm[ts_data_lstm['lstm_anomaly']]['value'],\n",
        "            color='magenta', marker='x', label='LSTM Detection')\n",
        "plt.title('LSTM Anomaly Detection')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualize LSTM errors\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data_lstm['timestamp'], lstm_errors, color='blue', label='Prediction Error')\n",
        "plt.axhline(y=lstm_threshold, color='red', linestyle='--', label='Threshold')\n",
        "plt.scatter(ts_data_lstm[ts_data_lstm['lstm_anomaly']]['timestamp'],\n",
        "            ts_data_lstm[ts_data_lstm['lstm_anomaly']]['lstm_error'],\n",
        "            color='magenta', marker='x', label='Detected Anomalies')\n",
        "plt.title('LSTM Prediction Errors with Anomaly Threshold')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Absolute Error')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualize the LSTM architecture with gate mechanisms\n",
        "def visualize_lstm_gates():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Create a diagram-like visualization\n",
        "    plt.plot([0, 1], [0.5, 0.5], 'k-', linewidth=2)  # Cell state line\n",
        "    plt.plot([0, 1], [0.2, 0.2], 'k--')  # Hidden state line\n",
        "\n",
        "    # Gates\n",
        "    plt.scatter([0.2, 0.4, 0.8], [0.5, 0.5, 0.5], s=400, color=['red', 'green', 'blue'], alpha=0.7)\n",
        "    plt.text(0.2, 0.5, 'f', color='white', ha='center', va='center', fontsize=14)\n",
        "    plt.text(0.4, 0.5, 'i', color='white', ha='center', va='center', fontsize=14)\n",
        "    plt.text(0.8, 0.5, 'o', color='white', ha='center', va='center', fontsize=14)\n",
        "\n",
        "    # Labels\n",
        "    plt.text(0, 0.55, '$C_{t-1}$', ha='left', va='bottom', fontsize=12)\n",
        "    plt.text(1, 0.55, '$C_t$', ha='right', va='bottom', fontsize=12)\n",
        "    plt.text(0, 0.25, '$h_{t-1}$', ha='left', va='bottom', fontsize=12)\n",
        "    plt.text(1, 0.25, '$h_t$', ha='right', va='bottom', fontsize=12)\n",
        "    plt.text(0.2, 0.6, 'Forget Gate', ha='center', va='bottom')\n",
        "    plt.text(0.4, 0.6, 'Input Gate', ha='center', va='bottom')\n",
        "    plt.text(0.8, 0.6, 'Output Gate', ha='center', va='bottom')\n",
        "\n",
        "    plt.title('LSTM Gate Mechanisms')\n",
        "    plt.xlim(-0.1, 1.1)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_lstm_gates()\n"
      ],
      "metadata": {
        "id": "A5gpsz9ErzWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. LSTM Autoencoders for Anomaly Detection\n",
        "\n",
        "Autoencoders are a type of neural network that learn to reconstruct their input. By training an autoencoder on normal data, it learns to reconstruct normal patterns well. When fed with anomalous data, the reconstruction error increases, providing a natural anomaly score.\n",
        "\n",
        "LSTM Autoencoders combine the sequence-handling capabilities of LSTMs with the unsupervised learning approach of autoencoders, making them particularly well-suited for time series anomaly detection.\n"
      ],
      "metadata": {
        "id": "E8vByxNbr3Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import RepeatVector, TimeDistributed\n",
        "\n",
        "# Build LSTM Autoencoder model\n",
        "def build_lstm_autoencoder(seq_length, features=1):\n",
        "    model = Sequential([\n",
        "        # Encoder\n",
        "        LSTM(32, activation='relu', input_shape=(seq_length, features), return_sequences=True),\n",
        "        LSTM(16, activation='relu', return_sequences=False),\n",
        "\n",
        "        # Bottleneck\n",
        "        RepeatVector(seq_length),  # Repeat the encoded representation for decoder\n",
        "\n",
        "        # Decoder\n",
        "        LSTM(16, activation='relu', return_sequences=True),\n",
        "        LSTM(32, activation='relu', return_sequences=True),\n",
        "\n",
        "        # Output layer\n",
        "        TimeDistributed(Dense(features))  # Output for each time step\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Create sequences for autoencoder (input = output)\n",
        "def create_autoencoder_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - seq_length + 1):\n",
        "        sequences.append(data[i:i+seq_length])\n",
        "    return np.array(sequences)\n",
        "\n",
        "# Prepare data for autoencoder\n",
        "autoencoder_sequences = create_autoencoder_sequences(data_scaled, seq_length)\n",
        "autoencoder_sequences_reshaped = autoencoder_sequences.reshape(-1, seq_length, 1)\n",
        "\n",
        "# Split into normal and anomaly sequences\n",
        "normal_indices = []\n",
        "anomaly_indices = []\n",
        "\n",
        "for i in range(len(autoencoder_sequences)):\n",
        "    # Check if there are any anomalies in this sequence\n",
        "    if np.any(ts_data['anomaly'][i:i+seq_length]):\n",
        "        anomaly_indices.append(i)\n",
        "    else:\n",
        "        normal_indices.append(i)\n",
        "\n",
        "# Get normal data for training\n",
        "normal_sequences = autoencoder_sequences_reshaped[normal_indices]\n",
        "\n",
        "# Split into train and validation\n",
        "train_val_split = int(len(normal_sequences) * 0.8)\n",
        "train_sequences = normal_sequences[:train_val_split]\n",
        "val_sequences = normal_sequences[train_val_split:]\n",
        "\n",
        "# Build and train autoencoder\n",
        "lstm_autoencoder = build_lstm_autoencoder(seq_length)\n",
        "lstm_autoencoder.summary()\n",
        "\n",
        "# Train the autoencoder on normal data only\n",
        "history_autoencoder = lstm_autoencoder.fit(\n",
        "    train_sequences, train_sequences,  # Input = output for autoencoder\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(val_sequences, val_sequences),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot autoencoder training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(history_autoencoder.history['loss'], label='Training Loss')\n",
        "plt.plot(history_autoencoder.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTM Autoencoder Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Calculate reconstruction errors\n",
        "def calculate_reconstruction_error(model, data):\n",
        "    \"\"\"Calculate sequence reconstruction error.\"\"\"\n",
        "    reconstructions = model.predict(data)\n",
        "    # MSE for each sequence\n",
        "    mse = np.mean(np.square(data - reconstructions), axis=(1, 2))\n",
        "    return mse\n",
        "\n",
        "# Get reconstruction errors\n",
        "train_errors = calculate_reconstruction_error(lstm_autoencoder, train_sequences)\n",
        "val_errors = calculate_reconstruction_error(lstm_autoencoder, val_sequences)\n",
        "all_errors = calculate_reconstruction_error(lstm_autoencoder, autoencoder_sequences_reshaped)\n",
        "\n",
        "# Set threshold based on validation errors\n",
        "threshold = np.mean(val_errors) + 3 * np.std(val_errors)\n",
        "print(f\"Reconstruction error threshold: {threshold:.6f}\")\n",
        "\n",
        "# Detect anomalies\n",
        "autoencoder_anomalies = all_errors > threshold\n",
        "\n",
        "# Map back to original data\n",
        "ts_data_ae = ts_data.copy()\n",
        "ts_data_ae['ae_anomaly'] = False\n",
        "ts_data_ae['reconstruction_error'] = np.nan\n",
        "\n",
        "# For each sequence, if it's anomalous, mark all points in that sequence\n",
        "for i in range(len(autoencoder_anomalies)):\n",
        "    if autoencoder_anomalies[i]:\n",
        "        ts_data_ae.loc[i:i+seq_length-1, 'ae_anomaly'] = True\n",
        "    ts_data_ae.loc[i, 'reconstruction_error'] = all_errors[i]\n",
        "\n",
        "# Calculate metrics for autoencoder method\n",
        "ae_metrics = calculate_metrics(ts_data_ae['anomaly'], ts_data_ae['ae_anomaly'], \"LSTM Autoencoder\")\n",
        "\n",
        "# Visualize autoencoder results\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data_ae['timestamp'], ts_data_ae['value'], color='blue', label='Data')\n",
        "plt.scatter(ts_data_ae[ts_data_ae['anomaly']]['timestamp'],\n",
        "            ts_data_ae[ts_data_ae['anomaly']]['value'],\n",
        "            color='red', marker='o', label='True Anomaly')\n",
        "plt.scatter(ts_data_ae[ts_data_ae['ae_anomaly']]['timestamp'],\n",
        "            ts_data_ae[ts_data_ae['ae_anomaly']]['value'],\n",
        "            color='cyan', marker='x', label='Autoencoder Detection')\n",
        "plt.title('LSTM Autoencoder Anomaly Detection')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualize reconstruction errors\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_data_ae['timestamp'], ts_data_ae['reconstruction_error'], color='blue', label='Reconstruction Error')\n",
        "plt.axhline(y=threshold, color='red', linestyle='--', label='Threshold')\n",
        "plt.title('LSTM Autoencoder Reconstruction Errors')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Compare normal vs anomaly reconstructions\n",
        "def visualize_reconstructions():\n",
        "    # Get sample normal sequence\n",
        "    normal_idx = normal_indices[10]\n",
        "    normal_seq = autoencoder_sequences_reshaped[normal_idx]\n",
        "    normal_recon = lstm_autoencoder.predict(normal_seq.reshape(1, seq_length, 1))[0]\n",
        "\n",
        "    # Get sample anomaly sequence\n",
        "    anomaly_idx = anomaly_indices[5]\n",
        "    anomaly_seq = autoencoder_sequences_reshaped[anomaly_idx]\n",
        "    anomaly_recon = lstm_autoencoder.predict(anomaly_seq.reshape(1, seq_length, 1))[0]\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "    # Normal sequence\n",
        "    axes[0].plot(normal_seq, label='Original', color='blue')\n",
        "    axes[0].plot(normal_recon, label='Reconstruction', color='green', linestyle='--')\n",
        "    normal_error = np.mean(np.square(normal_seq - normal_recon))\n",
        "    axes[0].set_title(f'Normal Sequence (MSE: {normal_error:.6f})')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Anomaly sequence\n",
        "    axes[1].plot(anomaly_seq, label='Original', color='blue')\n",
        "    axes[1].plot(anomaly_recon, label='Reconstruction', color='red', linestyle='--')\n",
        "    anomaly_error = np.mean(np.square(anomaly_seq - anomaly_recon))\n",
        "    axes[1].set_title(f'Anomalous Sequence (MSE: {anomaly_error:.6f})')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_reconstructions()\n"
      ],
      "metadata": {
        "id": "7H7QJP9sr5fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Comparison of Methods\n",
        "\n",
        "Let's compare the performance of all the anomaly detection methods we've implemented:\n"
      ],
      "metadata": {
        "id": "hQVRXLaEr9I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a comparison DataFrame\n",
        "methods = ['Moving Window', 'ARIMA', 'Feedforward NN', 'LSTM', 'LSTM Autoencoder']\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Method': methods,\n",
        "    'Accuracy': [mw_metrics['accuracy'], arima_metrics['accuracy'],\n",
        "                ffnn_metrics['accuracy'], lstm_metrics['accuracy'], ae_metrics['accuracy']],\n",
        "    'Precision': [mw_metrics['precision'], arima_metrics['precision'],\n",
        "                 ffnn_metrics['precision'], lstm_metrics['precision'], ae_metrics['precision']],\n",
        "    'Recall': [mw_metrics['recall'], arima_metrics['recall'],\n",
        "               ffnn_metrics['recall'], lstm_metrics['recall'], ae_metrics['recall']],\n",
        "    'F1 Score': [mw_metrics['f1'], arima_metrics['f1'],\n",
        "                ffnn_metrics['f1'], lstm_metrics['f1'], ae_metrics['f1']]\n",
        "})\n",
        "\n",
        "# Display comparison table\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# Create bar chart comparison\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Plot each metric\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "width = 0.2\n",
        "x = np.arange(len(methods))\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    plt.bar(x + i*width - width*1.5, metrics_df[metric], width, label=metric)\n",
        "\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Comparison of Anomaly Detection Methods')\n",
        "plt.xticks(x, methods)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r1xtQYvVr-7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "In this notebook, we've explored a range of approaches for anomaly detection in time series data:\n",
        "\n",
        "### Statistical Methods:\n",
        "- **Moving Window**: Simple and interpretable, but less effective for complex patterns\n",
        "- **ARIMA**: Captures time series dynamics, but requires parameter tuning and stationarity\n",
        "\n",
        "### Neural Network Approaches:\n",
        "- **Feedforward Neural Networks**: Can learn non-linear patterns using windowed data\n",
        "- **Recurrent Neural Networks**: Designed for sequences, but suffer from vanishing gradients\n",
        "- **LSTM**: Addresses the vanishing gradient problem with gate mechanisms\n",
        "- **LSTM Autoencoders**: Unsupervised approach that learns normal patterns and detects deviations\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "1. **Context matters**: Different anomaly types require different detection approaches\n",
        "2. **Trade-offs**: More complex models generally perform better but require more data and computational resources\n",
        "3. **Interpretability**: Statistical methods provide clearer explanations but may miss subtle patterns\n",
        "4. **Unsupervised learning**: LSTM autoencoders excel when labeled anomaly data is scarce\n",
        "\n",
        "For real-world applications, consider:\n",
        "- Combining multiple methods for robust detection\n",
        "- Careful threshold selection based on domain knowledge\n",
        "- Regular model retraining as normal patterns evolve over time\n"
      ],
      "metadata": {
        "id": "tVl5GYDnsBXl"
      }
    }
  ]
}