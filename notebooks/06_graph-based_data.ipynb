{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMKUUGuAgyPIgYMUIsvQCkC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Graph-Based Anomaly Detection: From Theory to Practice\n","\n","Welcome to this comprehensive exploration of anomaly detection in graph-structured data. In this notebook, we'll journey from fundamental graph concepts to advanced deep learning techniques for identifying anomalous nodes, edges, and subgraphs.\n","\n","## What You'll Learn\n","- Essential graph theory concepts and representations\n","- Statistical approaches to graph anomaly detection\n","- Graph embedding techniques (DeepWalk, Node2Vec)\n","- Graph Neural Networks (GNNs) architecture and applications\n","- Graph Convolutional Networks (GCNs) for anomaly detection\n","- Advanced GCN autoencoder models for unsupervised anomaly detection\n","\n","By the end, you'll understand both the theoretical foundations and practical implementations of these methods using popular libraries like NetworkX, node2vec, and PyTorch Geometric.\n"],"metadata":{"id":"L_dYfbJWvJgv"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install networkx numpy matplotlib pandas scikit-learn node2vec torch torch-geometric torch-scatter torch-sparse\n","\n","# Import common libraries we'll use throughout the notebook\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","import torch\n","import random\n","import warnings\n","\n","# Set seeds for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)\n","random.seed(42)\n","warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ra4yJCA1vN6h","outputId":"bb5782da-b2f2-4ea9-b7ef-3e3de0c3563e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Collecting node2vec\n","  Using cached node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting torch-geometric\n","  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","Collecting torch-scatter\n","  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-sparse\n","  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Collecting gensim<5.0.0,>=4.3.0 (from node2vec)\n","  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting numpy\n","  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from node2vec) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n","Collecting scipy>=1.6.0 (from scikit-learn)\n","  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.17.2)\n","Using cached node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n","Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","Building wheels for collected packages: torch-scatter, torch-sparse\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3622728 sha256=16dfb6b6eddcd01e476b59ae9fc5902aea9f23c40528f7dbe5ae188b71e5242a\n","  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n"]}]},{"cell_type":"markdown","source":["## 1. Graph Fundamentals\n","\n","### 1.1 What is a Graph?\n","\n","A graph is a mathematical structure consisting of:\n","- **Nodes** (also called vertices): Representing entities\n","- **Edges**: Representing relationships between entities\n","\n","Graphs are powerful data structures because they explicitly model relationships rather than treating data points as independent.\n","\n","### 1.2 Types of Graphs\n","\n","- **Undirected Graph**: Edges have no direction (friendship network)\n","- **Directed Graph**: Edges have direction (follower relationships)\n","- **Weighted Graph**: Edges have weights (distances, strengths)\n","- **Attributed Graph**: Nodes and edges have features (user profiles, transaction details)\n","- **Dynamic Graph**: Graph structure evolves over time (communication networks)\n"],"metadata":{"id":"WzDeW7dsvQVv"}},{"cell_type":"code","source":["# Let's create and visualize different types of graphs\n","\n","# Create plots for different graph types\n","fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","# 1. Undirected graph\n","G_undirected = nx.Graph()\n","G_undirected.add_edges_from([(1, 2), (1, 3), (2, 3), (3, 4), (4, 5), (4, 6), (5, 6)])\n","\n","pos_undirected = nx.spring_layout(G_undirected, seed=42)\n","nx.draw(G_undirected, pos_undirected, with_labels=True, node_color='skyblue',\n","        node_size=500, ax=axes[0])\n","axes[0].set_title('Undirected Graph', fontsize=14)\n","\n","# 2. Directed graph\n","G_directed = nx.DiGraph()\n","G_directed.add_edges_from([(1, 2), (2, 3), (3, 1), (3, 4), (4, 5), (5, 6), (6, 4)])\n","\n","pos_directed = nx.spring_layout(G_directed, seed=42)\n","nx.draw(G_directed, pos_directed, with_labels=True, node_color='lightgreen',\n","        node_size=500, ax=axes[1], arrows=True, arrowsize=15)\n","axes[1].set_title('Directed Graph', fontsize=14)\n","\n","# 3. Weighted graph\n","G_weighted = nx.Graph()\n","weighted_edges = [(1, 2, 0.8), (1, 3, 0.3), (2, 3, 0.9),\n","                  (3, 4, 1.2), (4, 5, 0.5), (5, 6, 0.7), (4, 6, 2.0)]\n","G_weighted.add_weighted_edges_from(weighted_edges)\n","\n","pos_weighted = nx.spring_layout(G_weighted, seed=42)\n","nx.draw(G_weighted, pos_weighted, with_labels=True, node_color='salmon',\n","        node_size=500, ax=axes[2])\n","\n","# Add edge labels for weights\n","edge_labels = nx.get_edge_attributes(G_weighted, 'weight')\n","nx.draw_networkx_edge_labels(G_weighted, pos_weighted, edge_labels=edge_labels, ax=axes[2])\n","axes[2].set_title('Weighted Graph', fontsize=14)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"u7LPpouSvS7k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.3 Graph Representations\n","\n","Graphs can be represented in several ways:\n","\n","1. **Adjacency Matrix**: An n×n matrix where n is the number of nodes. Entry A[i,j] = 1 if there's an edge from node i to node j, otherwise 0.\n","   - Pros: Simple for dense graphs, efficient for operations like matrix multiplication\n","   - Cons: Memory inefficient for sparse graphs\n","\n","2. **Adjacency List**: For each node, store a list of its neighboring nodes.\n","   - Pros: Memory efficient for sparse graphs\n","   - Cons: Less efficient for checking if two nodes are connected\n","\n","3. **Edge List**: Simply a list of edges (i,j).\n","   - Pros: Most compact representation\n","   - Cons: Inefficient for many operations\n"],"metadata":{"id":"go7VzFg1vVe3"}},{"cell_type":"code","source":["# Let's create visual representations of these formats for the same graph\n","\n","# Create a simple graph\n","G = nx.Graph()\n","G.add_edges_from([(0, 1), (0, 2), (1, 2), (2, 3), (3, 4), (4, 5), (4, 6), (5, 6)])\n","\n","# Visualize the graph\n","plt.figure(figsize=(6, 5))\n","pos = nx.spring_layout(G, seed=42)\n","nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500)\n","plt.title('Example Graph', fontsize=14)\n","plt.show()\n","\n","# Display the different representations\n","print(\"1. Adjacency Matrix:\")\n","A = nx.adjacency_matrix(G).todense()\n","print(A)\n","print(\"\\n2. Adjacency List:\")\n","for node in G.nodes():\n","    print(f\"Node {node}: {list(G.neighbors(node))}\")\n","print(\"\\n3. Edge List:\")\n","print(list(G.edges()))\n","\n","# For later use, we'll need functions to convert between formats\n","def adj_matrix_to_edge_list(A):\n","    \"\"\"Convert adjacency matrix to edge list\"\"\"\n","    edge_list = []\n","    n = A.shape[0]\n","    for i in range(n):\n","        for j in range(n):\n","            if A[i, j] != 0:\n","                edge_list.append((i, j, A[i, j]))\n","    return edge_list\n","\n","def edge_list_to_adj_matrix(edge_list, n_nodes):\n","    \"\"\"Convert edge list to adjacency matrix\"\"\"\n","    A = np.zeros((n_nodes, n_nodes))\n","    for edge in edge_list:\n","        i, j, w = edge\n","        A[i, j] = w\n","        if not nx.is_directed(G):  # If undirected, make symmetric\n","            A[j, i] = w\n","    return A\n"],"metadata":{"id":"Irp4nKX5vYFd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.4 Graph Measures and Properties\n","\n","To detect anomalies in graphs, we need to understand what \"normal\" looks like. Here are some key measures:\n","\n","#### Node-level Metrics:\n","- **Degree**: Number of connections a node has\n","- **Clustering coefficient**: How connected a node's neighbors are to each other\n","- **Centrality measures**:\n","  - Betweenness: How often a node lies on shortest paths between other nodes\n","  - Closeness: Average shortest path distance to all other nodes\n","  - Eigenvector: Node importance based on importance of its connections\n","\n","#### Graph-level Metrics:\n","- **Density**: Ratio of actual edges to possible edges\n","- **Diameter**: Length of the longest shortest path\n","- **Average path length**: Average shortest path between all pairs of nodes\n","- **Community structure**: Groups of nodes densely connected internally\n"],"metadata":{"id":"lFR-oMK2va3v"}},{"cell_type":"code","source":["# Let's calculate these metrics for our graph\n","\n","# Create a larger graph to demonstrate metrics\n","G_metrics = nx.barabasi_albert_graph(20, 3, seed=42)  # Scale-free graph\n","\n","# Calculate node-level metrics\n","degrees = dict(G_metrics.degree())\n","clustering = nx.clustering(G_metrics)\n","betweenness = nx.betweenness_centrality(G_metrics)\n","closeness = nx.closeness_centrality(G_metrics)\n","eigenvector = nx.eigenvector_centrality(G_metrics)\n","\n","# Combine into a DataFrame for easier viewing\n","metrics_df = pd.DataFrame({\n","    'Degree': pd.Series(degrees),\n","    'Clustering': pd.Series(clustering),\n","    'Betweenness': pd.Series(betweenness),\n","    'Closeness': pd.Series(closeness),\n","    'Eigenvector': pd.Series(eigenvector)\n","})\n","\n","# Calculate graph-level metrics\n","density = nx.density(G_metrics)\n","try:\n","    diameter = nx.diameter(G_metrics)\n","except:\n","    diameter = \"Graph is not connected\"\n","avg_path = nx.average_shortest_path_length(G_metrics)\n","avg_clustering = nx.average_clustering(G_metrics)\n","\n","# Show results\n","print(\"Node-level Metrics (first 5 nodes):\")\n","print(metrics_df.head(5))\n","\n","print(\"\\nGraph-level Metrics:\")\n","print(f\"Density: {density:.4f}\")\n","print(f\"Diameter: {diameter}\")\n","print(f\"Average path length: {avg_path:.4f}\")\n","print(f\"Average clustering coefficient: {avg_clustering:.4f}\")\n","\n","# Visualize the graph with node size proportional to degree\n","plt.figure(figsize=(10, 8))\n","pos = nx.spring_layout(G_metrics, seed=42)\n","node_size = [v * 100 for v in degrees.values()]\n","node_color = list(betweenness.values())\n","nx.draw(G_metrics, pos, with_labels=True,\n","        node_size=node_size, node_color=node_color,\n","        cmap='viridis', edge_color='gray', alpha=0.8)\n","plt.title('Graph Visualization with Node Size = Degree, Color = Betweenness', fontsize=14)\n","plt.colorbar(plt.cm.ScalarMappable(cmap='viridis'))\n","plt.show()\n"],"metadata":{"id":"LCJrSZc8vdUh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Creating a Graph Dataset for Anomaly Detection\n","\n","Let's create a synthetic graph dataset that models a financial transaction network with some fraudulent patterns injected. This will serve as our example throughout the notebook.\n","\n","In financial transaction networks:\n","- Nodes represent accounts or entities\n","- Edges represent transactions between accounts\n","- Edge weights might represent transaction amounts\n","- Node features might include account information\n"],"metadata":{"id":"4avsdZNnvfqT"}},{"cell_type":"code","source":["# Create a financial transaction network with injected anomalies\n","def create_transaction_network(n_accounts=100, n_transactions=500, n_features=8,\n","                              n_fraudsters=5, anomaly_types=['structural', 'attribute', 'combined']):\n","    \"\"\"\n","    Create a synthetic financial transaction network with injected anomalies.\n","\n","    Parameters:\n","    -----------\n","    n_accounts : int\n","        Number of accounts (nodes) in the network\n","    n_transactions : int\n","        Number of normal transactions (edges)\n","    n_features : int\n","        Number of features per account\n","    n_fraudsters : int\n","        Number of fraudulent accounts to inject\n","    anomaly_types : list\n","        Types of anomalies to inject\n","\n","    Returns:\n","    --------\n","    G : networkx.Graph\n","        Transaction network with anomalies\n","    anomaly_nodes : list\n","        List of anomalous node indices\n","    anomaly_edges : list\n","        List of anomalous edge tuples\n","    \"\"\"\n","    # Create an empty graph\n","    G = nx.Graph()\n","\n","    # Add normal accounts with features\n","    for i in range(n_accounts):\n","        # Generate normal account features (e.g., avg balance, account age, etc.)\n","        features = np.random.normal(0, 1, n_features)\n","        G.add_node(i, features=features, anomaly=False, anomaly_type=None)\n","\n","    # Add normal transactions\n","    for _ in range(n_transactions):\n","        source = np.random.randint(0, n_accounts)\n","        target = np.random.randint(0, n_accounts)\n","        if source != target:  # Avoid self-loops\n","            amount = np.random.lognormal(mean=3, sigma=1)  # Transaction amount\n","            G.add_edge(source, target, weight=amount, anomaly=False)\n","\n","    # Inject anomalies\n","    anomaly_nodes = []\n","    anomaly_edges = []\n","\n","    # 1. Structural anomalies: Unusual connectivity patterns\n","    if 'structural' in anomaly_types:\n","        # Create a fraudster with unusually high out-degree (many small transactions)\n","        fraudster = n_accounts\n","        G.add_node(fraudster, features=np.random.normal(0, 1, n_features),\n","                  anomaly=True, anomaly_type='structural')\n","        for _ in range(20):  # Many outgoing transactions\n","            target = np.random.randint(0, n_accounts)\n","            amount = np.random.lognormal(mean=1, sigma=0.2)  # Small amounts\n","            G.add_edge(fraudster, target, weight=amount, anomaly=True)\n","            anomaly_edges.append((fraudster, target))\n","        anomaly_nodes.append(fraudster)\n","        n_accounts += 1\n","\n","        # Create a circular transaction pattern (money laundering)\n","        start_idx = n_accounts\n","        circle_size = 5\n","        for i in range(circle_size):\n","            node_idx = start_idx + i\n","            G.add_node(node_idx, features=np.random.normal(0, 1, n_features),\n","                      anomaly=True, anomaly_type='structural')\n","            anomaly_nodes.append(node_idx)\n","\n","        # Add circular edges\n","        for i in range(circle_size):\n","            source = start_idx + i\n","            target = start_idx + (i + 1) % circle_size\n","            G.add_edge(source, target, weight=np.random.lognormal(mean=4, sigma=0.1),\n","                      anomaly=True)\n","            anomaly_edges.append((source, target))\n","\n","        n_accounts += circle_size\n","\n","    # 2. Attribute anomalies: Unusual feature values\n","    if 'attribute' in anomaly_types:\n","        # Create accounts with unusual feature values\n","        for i in range(n_fraudsters - len(anomaly_nodes)):\n","            node_idx = n_accounts + i\n","            # Very unusual feature values (large deviation)\n","            features = np.random.normal(5, 2, n_features)\n","            G.add_node(node_idx, features=features, anomaly=True,\n","                      anomaly_type='attribute')\n","\n","            # Add some normal-looking transactions\n","            for _ in range(np.random.randint(2, 6)):\n","                target = np.random.randint(0, n_accounts)\n","                amount = np.random.lognormal(mean=3, sigma=1)\n","                G.add_edge(node_idx, target, weight=amount, anomaly=False)\n","\n","            anomaly_nodes.append(node_idx)\n","\n","        n_accounts += n_fraudsters - len(anomaly_nodes)\n","\n","    # 3. Combined anomalies: Both structural and attribute irregularities\n","    if 'combined' in anomaly_types:\n","        # Create a densely connected group with unusual features\n","        group_size = 4\n","        start_idx = n_accounts\n","\n","        # Create nodes with unusual features\n","        for i in range(group_size):\n","            node_idx = start_idx + i\n","            features = np.random.normal(3, 1, n_features)\n","            # Last feature is highly anomalous\n","            features[-1] = np.random.uniform(8, 10)\n","            G.add_node(node_idx, features=features, anomaly=True,\n","                      anomaly_type='combined')\n","            anomaly_nodes.append(node_idx)\n","\n","        # Create dense connections within group\n","        for i in range(group_size):\n","            for j in range(i+1, group_size):\n","                source = start_idx + i\n","                target = start_idx + j\n","                G.add_edge(source, target, weight=np.random.lognormal(mean=5, sigma=0.5),\n","                          anomaly=True)\n","                anomaly_edges.append((source, target))\n","\n","        n_accounts += group_size\n","\n","    print(f\"Created graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n","    print(f\"Injected {len(anomaly_nodes)} anomalous nodes and {len(anomaly_edges)} anomalous edges\")\n","\n","    return G, anomaly_nodes, anomaly_edges\n","\n","# Create our transaction network\n","G_financial, anomaly_nodes, anomaly_edges = create_transaction_network()\n","\n","# Visualize the network\n","plt.figure(figsize=(12, 10))\n","pos = nx.spring_layout(G_financial, seed=42)\n","\n","# Draw normal nodes and edges\n","normal_nodes = [n for n in G_financial.nodes() if n not in anomaly_nodes]\n","nx.draw_networkx_nodes(G_financial, pos, nodelist=normal_nodes,\n","                      node_color='skyblue', node_size=50)\n","nx.draw_networkx_nodes(G_financial, pos, nodelist=anomaly_nodes,\n","                      node_color='red', node_size=80)\n","\n","# Draw edges with width proportional to weight\n","edge_widths = [0.1 + 0.2 * G_financial[u][v]['weight']/10 for u, v in G_financial.edges()]\n","\n","# Normal edges\n","normal_edges = [(u, v) for u, v in G_financial.edges() if (u, v) not in anomaly_edges and (v, u) not in anomaly_edges]\n","nx.draw_networkx_edges(G_financial, pos, edgelist=normal_edges,\n","                      width=edge_widths[:len(normal_edges)], alpha=0.3)\n","\n","# Anomalous edges\n","nx.draw_networkx_edges(G_financial, pos, edgelist=anomaly_edges,\n","                      edge_color='red', width=1, alpha=0.7)\n","\n","plt.title('Financial Transaction Network with Injected Anomalies', fontsize=16)\n","plt.legend(['Normal Accounts', 'Fraudulent Accounts', 'Normal Transactions', 'Fraudulent Transactions'])\n","plt.axis('off')\n","plt.show()\n"],"metadata":{"id":"PuHnXKA7vh4K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Traditional Graph Anomaly Detection Methods\n","\n","Traditional methods for graph anomaly detection typically rely on statistical properties of the graph structure. These methods are computationally efficient but may miss complex patterns.\n","\n","We'll explore approaches based on:\n","1. **Node metrics**: Finding nodes with unusual statistics\n","2. **Community detection**: Identifying nodes that don't fit well in communities\n","3. **Spectral methods**: Using eigenvectors of the graph matrices\n"],"metadata":{"id":"z8vwHyObvl-x"}},{"cell_type":"code","source":["# 3.1 Statistical Anomaly Detection\n","\n","def statistical_anomaly_detection(G, metrics=['degree', 'clustering', 'betweenness'], threshold=2.5):\n","    \"\"\"\n","    Detect anomalies based on statistical properties of nodes.\n","\n","    Parameters:\n","    -----------\n","    G : networkx.Graph\n","        The input graph\n","    metrics : list\n","        Metrics to consider\n","    threshold : float\n","        Z-score threshold for anomaly detection\n","\n","    Returns:\n","    --------\n","    anomalies : dict\n","        Dictionary mapping metrics to lists of anomalous nodes\n","    scores : dict\n","        Dictionary mapping nodes to their anomaly scores\n","    \"\"\"\n","    results = {}\n","    scores = {node: 0 for node in G.nodes()}\n","\n","    # Calculate metrics\n","    if 'degree' in metrics:\n","        degrees = dict(G.degree())\n","        degree_values = list(degrees.values())\n","        deg_mean = np.mean(degree_values)\n","        deg_std = np.std(degree_values) if np.std(degree_values) > 0 else 1\n","\n","        # Calculate z-scores\n","        degree_z = {node: abs((degrees[node] - deg_mean) / deg_std) for node in G.nodes()}\n","        degree_anomalies = [node for node, z in degree_z.items() if z > threshold]\n","        results['degree'] = degree_anomalies\n","\n","        # Update scores\n","        for node in G.nodes():\n","            scores[node] += degree_z[node]\n","\n","    if 'clustering' in metrics:\n","        clustering = nx.clustering(G)\n","        clustering_values = list(clustering.values())\n","        clust_mean = np.mean(clustering_values)\n","        clust_std = np.std(clustering_values) if np.std(clustering_values) > 0 else 1\n","\n","        # Calculate z-scores\n","        clustering_z = {node: abs((clustering[node] - clust_mean) / clust_std) for node in G.nodes()}\n","        clustering_anomalies = [node for node, z in clustering_z.items() if z > threshold]\n","        results['clustering'] = clustering_anomalies\n","\n","        # Update scores\n","        for node in G.nodes():\n","            scores[node] += clustering_z[node]\n","\n","    if 'betweenness' in metrics:\n","        betweenness = nx.betweenness_centrality(G)\n","        betweenness_values = list(betweenness.values())\n","        bet_mean = np.mean(betweenness_values)\n","        bet_std = np.std(betweenness_values) if np.std(betweenness_values) > 0 else 1\n","\n","        # Calculate z-scores\n","        betweenness_z = {node: abs((betweenness[node] - bet_mean) / bet_std) for node in G.nodes()}\n","        betweenness_anomalies = [node for node, z in betweenness_z.items() if z > threshold]\n","        results['betweenness'] = betweenness_anomalies\n","\n","        # Update scores\n","        for node in G.nodes():\n","            scores[node] += betweenness_z[node]\n","\n","    # Normalize scores by number of metrics\n","    for node in scores:\n","        scores[node] /= len(metrics)\n","\n","    # Combined anomalies (anomalous in multiple metrics)\n","    all_anomalies = set()\n","    for metric_anomalies in results.values():\n","        all_anomalies.update(metric_anomalies)\n","    results['combined'] = list(all_anomalies)\n","\n","    return results, scores\n","\n","# Apply statistical anomaly detection to our financial network\n","stat_anomalies, stat_scores = statistical_anomaly_detection(G_financial)\n","\n","# Evaluate detection accuracy\n","def evaluate_anomaly_detection(detected_nodes, true_anomalies):\n","    \"\"\"Calculate precision, recall, and F1 score for anomaly detection\"\"\"\n","    true_positives = len(set(detected_nodes).intersection(set(true_anomalies)))\n","    precision = true_positives / len(detected_nodes) if len(detected_nodes) > 0 else 0\n","    recall = true_positives / len(true_anomalies) if len(true_anomalies) > 0 else 0\n","    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","    return precision, recall, f1\n","\n","print(\"Statistical Method Results:\")\n","print(f\"True anomalies: {len(anomaly_nodes)}\")\n","print(f\"Detected anomalies: {len(stat_anomalies['combined'])}\")\n","print(f\"Detected by degree: {len(stat_anomalies['degree'])}\")\n","print(f\"Detected by clustering: {len(stat_anomalies['clustering'])}\")\n","print(f\"Detected by betweenness: {len(stat_anomalies['betweenness'])}\")\n","print(\"\\nPerformance:\")\n","stat_performance = evaluate_anomaly_detection(stat_anomalies['combined'], anomaly_nodes)\n","\n","# Visualize the results\n","plt.figure(figsize=(12, 10))\n","pos = nx.spring_layout(G_financial, seed=42)\n","\n","# Color nodes by anomaly score\n","node_colors = [stat_scores[node] for node in G_financial.nodes()]\n","\n","# Draw nodes with color based on anomaly score\n","nodes = nx.draw_networkx_nodes(G_financial, pos, node_color=node_colors,\n","                               cmap='YlOrRd', node_size=50)\n","plt.colorbar(nodes, label='Anomaly Score')\n","\n","# Mark true anomalies with red border\n","true_anomaly_nodes = nx.draw_networkx_nodes(G_financial, pos, nodelist=anomaly_nodes,\n","                                           node_color='none', node_size=100,\n","                                           edgecolors='red', linewidths=2)\n","\n","# Draw detected anomalies with blue border\n","detected_nodes = nx.draw_networkx_nodes(G_financial, pos, nodelist=stat_anomalies['combined'],\n","                                       node_color='none', node_size=75,\n","                                       edgecolors='blue', linewidths=1.5)\n","\n","# Draw edges\n","nx.draw_networkx_edges(G_financial, pos, alpha=0.2, width=0.5)\n","\n","plt.title('Statistical Anomaly Detection Results', fontsize=16)\n","plt.legend([true_anomaly_nodes, detected_nodes], ['True Anomalies', 'Detected Anomalies'])\n","plt.axis('off')\n","plt.show()\n"],"metadata":{"id":"0Qo4qY9zvoEN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2 Community-Based Anomaly Detection\n","\n","Communities in graphs are groups of nodes that are densely connected internally but sparsely connected to other groups. Nodes that don't fit well into any community or connect multiple communities in unusual ways may be anomalous.\n"],"metadata":{"id":"312Z2DmzvrZ-"}},{"cell_type":"code","source":["# Community-based anomaly detection\n","def community_anomaly_detection(G, threshold=0.7):\n","    \"\"\"\n","    Detect anomalies based on community structure.\n","\n","    Parameters:\n","    -----------\n","    G : networkx.Graph\n","        The input graph\n","    threshold : float\n","        Threshold for community membership strength\n","\n","    Returns:\n","    --------\n","    anomalies : list\n","        List of anomalous nodes\n","    scores : dict\n","        Dictionary mapping nodes to their anomaly scores\n","    \"\"\"\n","    # Detect communities using Louvain method\n","    try:\n","        import community as community_louvain\n","        communities = community_louvain.best_partition(G)\n","    except ImportError:\n","        # Fallback to networkx implementation\n","        communities = nx.community.louvain_communities(G)\n","        communities = {node: i for i, comm in enumerate(communities) for node in comm}\n","\n","    # Count number of communities\n","    n_communities = len(set(communities.values()))\n","    print(f\"Detected {n_communities} communities\")\n","\n","    # Create a dictionary mapping community ID to nodes in that community\n","    community_dict = {}\n","    for node, comm_id in communities.items():\n","        if comm_id not in community_dict:\n","            community_dict[comm_id] = []\n","        community_dict[comm_id].append(node)\n","\n","    # For each node, calculate what fraction of its neighbors are in the same community\n","    scores = {}\n","    for node in G.nodes():\n","        neighbors = list(G.neighbors(node))\n","        if len(neighbors) == 0:\n","            scores[node] = 0  # Isolated nodes are not anomalous by this measure\n","            continue\n","\n","        # Count neighbors in same community\n","        node_comm = communities[node]\n","        same_comm_neighbors = sum(1 for neighbor in neighbors if communities[neighbor] == node_comm)\n","        comm_ratio = same_comm_neighbors / len(neighbors)\n","\n","        # Anomaly score = 1 - community ratio (higher means more anomalous)\n","        scores[node] = 1 - comm_ratio\n","\n","    # Nodes with high anomaly scores are anomalous\n","    anomalies = [node for node, score in scores.items() if score > threshold]\n","\n","    return anomalies, scores, communities\n","\n","# Apply community-based anomaly detection\n","comm_anomalies, comm_scores, communities = community_anomaly_detection(G_financial)\n","\n","print(\"\\nCommunity-Based Method Results:\")\n","print(f\"True anomalies: {len(anomaly_nodes)}\")\n","print(f\"Detected anomalies: {len(comm_anomalies)}\")\n","print(\"\\nPerformance:\")\n","comm_performance = evaluate_anomaly_detection(comm_anomalies, anomaly_nodes)\n","\n","# Visualize communities and anomalies\n","plt.figure(figsize=(12, 10))\n","pos = nx.spring_layout(G_financial, seed=42)\n","\n","# Color nodes by community\n","node_colors = [communities[node] for node in G_financial.nodes()]\n","\n","# Draw nodes with color based on community\n","nodes = nx.draw_networkx_nodes(G_financial, pos, node_color=node_colors,\n","                              cmap='tab20', node_size=50)\n","\n","# Draw edges\n","nx.draw_networkx_edges(G_financial, pos, alpha=0.2, width=0.5)\n","\n","# Mark detected anomalies with blue border\n","detected_nodes = nx.draw_networkx_nodes(G_financial, pos, nodelist=comm_anomalies,\n","                                       node_color='none', node_size=100,\n","                                       edgecolors='blue', linewidths=2)\n","\n","# Mark true anomalies with red border\n","true_anomaly_nodes = nx.draw_networkx_nodes(G_financial, pos, nodelist=anomaly_nodes,\n","                                           node_color='none', node_size=120,\n","                                           edgecolors='red', linewidths=2)\n","\n","plt.title('Community-Based Anomaly Detection Results', fontsize=16)\n","plt.legend([true_anomaly_nodes, detected_nodes], ['True Anomalies', 'Detected Anomalies'])\n","plt.axis('off')\n","plt.show()\n"],"metadata":{"id":"gVjg1uwWvt0I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.3 Spectral Anomaly Detection\n","\n","Spectral methods leverage the eigenvalues and eigenvectors of graph matrices like the adjacency matrix or Laplacian. These capture global structural properties of the graph.\n","\n","Nodes with unusual positions in the spectral embedding (eigenvectors corresponding to top eigenvalues) may be anomalous.\n"],"metadata":{"id":"ein9F7N9vw3F"}},{"cell_type":"code","source":["# Spectral anomaly detection\n","def spectral_anomaly_detection(G, n_components=2, threshold=2):\n","    \"\"\"\n","    Detect anomalies using spectral embedding.\n","\n","    Parameters:\n","    -----------\n","    G : networkx.Graph\n","        The input graph\n","    n_components : int\n","        Number of eigenvectors to use\n","    threshold : float\n","        Z-score threshold for considering points anomalous\n","\n","    Returns:\n","    --------\n","    anomalies : list\n","        List of anomalous nodes\n","    embedding : ndarray\n","        Spectral embedding of nodes\n","    \"\"\"\n","    # Compute adjacency matrix\n","    A = nx.to_numpy_array(G)\n","\n","    # Get spectral embedding (top eigenvectors)\n","    from sklearn.manifold import SpectralEmbedding\n","    embedding = SpectralEmbedding(n_components=n_components,\n","                                 affinity='precomputed').fit_transform(A)\n","\n","    # Calculate the distance of each point from the center\n","    center = np.mean(embedding, axis=0)\n","    distances = np.sqrt(np.sum((embedding - center)**2, axis=1))\n","\n","    # Z-score normalization of distances\n","    mean_dist = np.mean(distances)\n","    std_dist = np.std(distances)\n","    z_scores = (distances - mean_dist) / std_dist\n","\n","    # Nodes with high z-scores are anomalous\n","    anomalies = [i for i, z in enumerate(z_scores) if z > threshold]\n","\n","    return anomalies, embedding, z_scores\n","\n","# Apply spectral anomaly detection\n","spec_anomalies, spec_embedding, spec_scores = spectral_anomaly_detection(G_financial)\n","\n","print(\"\\nSpectral Method Results:\")\n","print(f\"True anomalies: {len(anomaly_nodes)}\")\n","print(f\"Detected anomalies: {len(spec_anomalies)}\")\n","print(\"\\nPerformance:\")\n","spec_performance = evaluate_anomaly_detection(spec_anomalies, anomaly_nodes)\n","\n","# Visualize spectral embedding\n","plt.figure(figsize=(10, 8))\n","\n","# Plot points colored by anomaly score\n","sc = plt.scatter(spec_embedding[:, 0], spec_embedding[:, 1],\n","                c=spec_scores, cmap='YlOrRd', s=50)\n","plt.colorbar(sc, label='Z-score')\n","\n","# Mark true anomalies with red circles\n","for node in anomaly_nodes:\n","    plt.scatter(spec_embedding[node, 0], spec_embedding[node, 1],\n","               s=150, facecolors='none', edgecolors='red', linewidths=2)\n","\n","# Mark detected anomalies with blue triangles\n","for node in spec_anomalies:\n","    plt.scatter(spec_embedding[node, 0], spec_embedding[node, 1],\n","               marker='^', s=100, facecolors='none', edgecolors='blue', linewidths=2)\n","\n","plt.title('Spectral Embedding of Graph Nodes', fontsize=16)\n","plt.legend(['Nodes', 'True Anomalies', 'Detected Anomalies'])\n","plt.grid(True, alpha=0.3)\n","plt.show()\n"],"metadata":{"id":"m3hqoX0tvzHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Graph Embeddings for Anomaly Detection\n","\n","Graph embedding methods map nodes to a continuous low-dimensional vector space, preserving graph topology. This allows us to apply traditional machine learning methods like clustering or outlier detection to the embeddings.\n","\n","### 4.1 Random Walk-Based Embeddings\n","\n","These methods generate random walks on the graph, treat walks as \"sentences,\" and use NLP techniques like Skip-gram to learn node representations.\n","\n","Key approaches include:\n","- **DeepWalk**: Uses uniform random walks\n","- **Node2Vec**: Uses biased random walks with parameters p (return) and q (in-out) to control walk strategy\n"],"metadata":{"id":"j_Jf3tWFv3Nr"}},{"cell_type":"code","source":["# First we need to install node2vec if not already installed\n","!pip install node2vec\n","\n","from node2vec import Node2Vec\n","\n","def node2vec_embedding(G, dimensions=64, walk_length=30, num_walks=200, p=1, q=1):\n","    \"\"\"\n","    Generate Node2Vec embeddings for graph nodes.\n","\n","    Parameters:\n","    -----------\n","    G : networkx.Graph\n","        The input graph\n","    dimensions : int\n","        Size of embeddings\n","    walk_length : int\n","        Length of each random walk\n","    num_walks : int\n","        Number of walks per node\n","    p : float\n","        Return parameter (controls likelihood of returning to previous node)\n","    q : float\n","        In-out parameter (controls DFS vs. BFS behavior)\n","\n","    Returns:\n","    --------\n","    embeddings : dict\n","        Dictionary mapping node IDs to embedding vectors\n","    model : node2vec.Word2Vec\n","        Trained embedding model\n","    \"\"\"\n","    # Precompute probabilities and generate walks\n","    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length,\n","                       num_walks=num_walks, p=p, q=q, workers=4)\n","\n","    # Train embeddings model\n","    model = node2vec.fit(window=10, min_count=1)\n","\n","    # Extract node embeddings\n","    embeddings = {}\n","    for node in G.nodes():\n","        embeddings[node] = model.wv[str(node)]\n","\n","    return embeddings, model\n","\n","# Anomaly detection using embeddings\n","def embedding_anomaly_detection(embeddings, method='isolation_forest', contamination=0.1):\n","    \"\"\"\n","    Detect anomalies in embeddings using outlier detection algorithms.\n","\n","    Parameters:\n","    -----------\n","    embeddings : dict\n","        Node embeddings\n","    method : str\n","        Detection method ('isolation_forest', 'lof', or 'ocsvm')\n","    contamination : float\n","        Expected proportion of outliers\n","\n","    Returns:\n","    --------\n","    anomalies : list\n","        List of anomalous nodes\n","    scores : array\n","        Anomaly scores\n","    \"\"\"\n","    # Convert embeddings to array\n","    nodes = list(embeddings.keys())\n","    X = np.array([embeddings[node] for node in nodes])\n","\n","    # Apply outlier detection method\n","    if method == 'isolation_forest':\n","        from sklearn.ensemble import IsolationForest\n","        detector = IsolationForest(contamination=contamination, random_state=42)\n","    elif method == 'lof':\n","        from sklearn.neighbors import LocalOutlierFactor\n","        detector = LocalOutlierFactor(n_neighbors=20, contamination=contamination)\n","    elif method == 'ocsvm':\n","        from sklearn.svm import OneClassSVM\n","        detector = OneClassSVM(nu=contamination)\n","\n","    # Fit and predict\n","    if method == 'lof':\n","        y_pred = detector.fit_predict(X)\n","        # LOF scores need to be negated and accessed differently\n","        scores = -detector.negative_outlier_factor_\n","    else:\n","        detector.fit(X)\n","        y_pred = detector.predict(X)\n","        scores = detector.decision_function(X)\n","\n","    # Get anomalies (labeled as -1)\n","    anomalies = [nodes[i] for i, label in enumerate(y_pred) if label == -1]\n","\n","    return anomalies, scores\n","\n","# Create Node2Vec embeddings with different parameters to see their effect\n","embeddings_configs = [\n","    {'p': 1, 'q': 1, 'name': 'Balanced'},  # Balanced between BFS and DFS\n","    {'p': 0.5, 'q': 2, 'name': 'Structural'}, # Favor structural equivalence\n","    {'p': 2, 'q': 0.5, 'name': 'Homophily'}  # Favor homophily\n","]\n","\n","# Dict to store results\n","embedding_results = {}\n","\n","for config in embeddings_configs:\n","    print(f\"Generating {config['name']} embeddings (p={config['p']}, q={config['q']})...\")\n","    embeddings, model = node2vec_embedding(G_financial, p=config['p'], q=config['q'], dimensions=32)\n","\n","    # Detect anomalies\n","    anomalies, scores = embedding_anomaly_detection(embeddings)\n","\n","    print(f\"Detected {len(anomalies)} anomalies\")\n","    print(\"Performance:\")\n","    performance = evaluate_anomaly_detection(anomalies, anomaly_nodes)\n","\n","    # Store results\n","    embedding_results[config['name']] = {\n","        'embeddings': embeddings,\n","        'anomalies': anomalies,\n","        'scores': scores,\n","        'performance': performance\n","    }\n"],"metadata":{"id":"nbk9DpJiv5eA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the embeddings using t-SNE\n","from sklearn.manifold import TSNE\n","\n","# Prepare figure for embedding visualization\n","fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n","\n","for i, config in enumerate(embeddings_configs):\n","    name = config['name']\n","    result = embedding_results[name]\n","    embeddings = result['embeddings']\n","\n","    # Convert embeddings to array for t-SNE\n","    nodes = list(embeddings.keys())\n","    X = np.array([embeddings[node] for node in nodes])\n","\n","    # Apply t-SNE for visualization\n","    X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\n","\n","    # Define node colors: true anomalies in red, detected in blue\n","    node_colors = ['lightgray'] * len(nodes)\n","    for j, node in enumerate(nodes):\n","        if node in anomaly_nodes:\n","            node_colors[j] = 'red'\n","        elif node in result['anomalies'] and node not in anomaly_nodes:\n","            node_colors[j] = 'blue'\n","\n","    # Plot embeddings\n","    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=node_colors, s=50, alpha=0.7)\n","\n","    # Highlight detected anomalies with blue circles\n","    for j, node in enumerate(nodes):\n","        if node in result['anomalies']:\n","            axes[i].scatter(X_tsne[j, 0], X_tsne[j, 1],\n","                          s=120, facecolors='none', edgecolors='blue', linewidths=1.5)\n","\n","    # Highlight true anomalies with red circles\n","    for j, node in enumerate(nodes):\n","        if node in anomaly_nodes:\n","            axes[i].scatter(X_tsne[j, 0], X_tsne[j, 1],\n","                          s=150, facecolors='none', edgecolors='red', linewidths=2)\n","\n","    axes[i].set_title(f\"{name} Embeddings (p={config['p']}, q={config['q']})\")\n","    axes[i].set_xticks([])\n","    axes[i].set_yticks([])\n","\n","# Create custom legend\n","from matplotlib.lines import Line2D\n","custom_lines = [\n","    Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgray', markersize=10),\n","    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10),\n","    Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10),\n","]\n","fig.legend(custom_lines, ['Normal', 'True Anomaly', 'False Positive'],\n","           loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0))\n","\n","plt.tight_layout()\n","plt.subplots_adjust(top=0.85)\n","plt.suptitle('Node2Vec Embeddings with Different Walk Strategies', fontsize=16)\n","plt.show()\n"],"metadata":{"id":"YQhZLOwGv-aQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.2 Skip-gram Model for Graph Embeddings\n","\n","The Skip-gram model is a neural network architecture used to learn distributed representations of nodes in a graph. It works by maximizing the probability of observing context nodes given a center node.\n","\n","For graph embeddings, we:\n","1. Generate random walks on the graph\n","2. For each node in a walk, predict its context nodes\n","3. Learn node embeddings that maximize this prediction accuracy\n","\n","This is analogous to how word2vec works in natural language processing, but with graph walks instead of sentences.\n"],"metadata":{"id":"rV50nX2hwBY0"}},{"cell_type":"code","source":["# Let's examine how node2vec internals work with Skip-gram\n","\n","def visualize_skipgram_concept():\n","    \"\"\"\n","    Visualize how Skip-gram works for graph embeddings\n","    \"\"\"\n","    # Create a small graph for demonstration\n","    G_small = nx.DiGraph()\n","    G_small.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4), (4, 0),\n","                           (0, 5), (5, 6), (6, 0)])\n","\n","    # Generate a random walk\n","    walk = [0, 1, 2, 3, 4, 0, 5, 6]\n","\n","    # Example window size\n","    window_size = 2\n","\n","    # Visualize the graph\n","    plt.figure(figsize=(12, 6))\n","\n","    # Plot graph on the left\n","    plt.subplot(1, 2, 1)\n","    pos = nx.spring_layout(G_small, seed=42)\n","    nx.draw(G_small, pos, with_labels=True, node_color='lightblue',\n","           node_size=500, arrows=True)\n","\n","    # Highlight the walk\n","    walk_edges = [(walk[i], walk[i+1]) for i in range(len(walk)-1)]\n","    nx.draw_networkx_edges(G_small, pos, edgelist=walk_edges, edge_color='red',\n","                          width=2)\n","\n","    plt.title('Random Walk on Graph')\n","\n","    # Plot Skip-gram concept on the right\n","    plt.subplot(1, 2, 2)\n","\n","    # Set up coordinates\n","    y_positions = [0, 1, 2, 3, 4, 5, 6, 7]\n","    x_positions = [2] * len(y_positions)\n","\n","    # Plot nodes\n","    plt.scatter(x_positions, y_positions, s=500, c='lightblue')\n","\n","    # Add node labels\n","    for i, node in enumerate(walk):\n","        plt.text(x_positions[i], y_positions[i], str(node),\n","                ha='center', va='center', fontsize=14)\n","\n","    # Mark center node and context for an example\n","    center_idx = 3  # Node 3 in the walk\n","    plt.scatter([x_positions[center_idx]], [y_positions[center_idx]],\n","               s=600, facecolors='none', edgecolors='red', linewidths=2)\n","\n","    # Connect to context nodes\n","    for j in range(max(0, center_idx-window_size), min(len(walk), center_idx+window_size+1)):\n","        if j != center_idx:\n","            plt.plot([x_positions[center_idx], x_positions[j]],\n","                    [y_positions[center_idx], y_positions[j]],\n","                    'r-', alpha=0.7)\n","\n","    plt.title('Skip-gram: Predict Context from Center')\n","    plt.xlim(0, 4)\n","    plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Show example training pairs\n","    print(\"Example Skip-gram training pairs (center → context):\")\n","    for i in range(len(walk)):\n","        center = walk[i]\n","        context = []\n","        for j in range(max(0, i-window_size), min(len(walk), i+window_size+1)):\n","            if j != i:\n","                context.append(walk[j])\n","        print(f\"Node {center} → Context {context}\")\n","\n","# Visualize Skip-gram concept\n","visualize_skipgram_concept()\n"],"metadata":{"id":"4M7OVRtbwDuQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Graph Neural Networks for Anomaly Detection\n","\n","Graph Neural Networks (GNNs) are deep learning models designed to capture the structure and attributes of graph data. Unlike graph embeddings which are fixed after training, GNNs can generalize to unseen nodes and graphs.\n","\n","### 5.1 GNN Fundamentals\n","\n","The key idea behind GNNs is **message passing**:\n","1. Nodes aggregate information from their neighbors\n","2. Node representations are updated based on this aggregated information\n","3. After multiple layers, each node's representation captures its neighborhood structure\n","\n","Graph Convolutional Networks (GCNs) are a popular type of GNN that generalize convolution operations to graph data.\n"],"metadata":{"id":"YnGtV6BGwGuW"}},{"cell_type":"code","source":["# We need PyTorch and PyTorch Geometric for GNN implementation\n","!pip install torch torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv, GATConv\n","from torch_geometric.data import Data\n","from torch_geometric.utils import from_networkx, to_networkx\n","\n","# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n"],"metadata":{"id":"rosb1LGOwVKU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.2 Setting Up Graph Data for GNNs\n","\n","To use graph data with GNNs, we need to convert our NetworkX graph to PyTorch Geometric's `Data` format, which includes:\n","- Node features\n","- Edge indices (COO format)\n","- Edge attributes (optional)\n","- Labels (for supervised learning)\n","\n","Let's prepare our financial transaction network for GNN processing:\n"],"metadata":{"id":"jX9X6g3dwYan"}},{"cell_type":"code","source":["# Create node features and edge index for PyTorch Geometric\n","def prepare_pyg_data(G, anomaly_nodes=None):\n","    \"\"\"\n","    Convert NetworkX graph to PyTorch Geometric Data object.\n","\n","    Parameters:\n","    -----------\n","    G : networkx.Graph\n","        Input graph\n","    anomaly_nodes : list\n","        List of known anomalous nodes for labels\n","\n","    Returns:\n","    --------\n","    data : torch_geometric.data.Data\n","        PyTorch Geometric Data object\n","    \"\"\"\n","    # Extract node features\n","    features = []\n","    for node in sorted(G.nodes()):\n","        if 'features' in G.nodes[node]:\n","            features.append(G.nodes[node]['features'])\n","        else:\n","            # If no features are available, use one-hot encoding of node ID\n","            one_hot = np.zeros(G.number_of_nodes())\n","            one_hot[node] = 1.0\n","            features.append(one_hot)\n","\n","    x = torch.tensor(np.array(features), dtype=torch.float)\n","\n","    # Create edge index (COO format)\n","    edge_index = []\n","    edge_attr = []\n","    for u, v, data in G.edges(data=True):\n","        # Each edge appears twice in undirected graph, indicating both directions\n","        edge_index.append([u, v])\n","        edge_index.append([v, u])  # For undirected graph\n","\n","        # Add edge attributes if available\n","        if 'weight' in data:\n","            edge_attr.append([data['weight']])\n","            edge_attr.append([data['weight']])  # Same weight for both directions\n","\n","    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n","\n","    if edge_attr:\n","        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n","    else:\n","        edge_attr = None\n","\n","    # Create labels (1 for anomaly, 0 for normal)\n","    if anomaly_nodes:\n","        y = torch.zeros(G.number_of_nodes(), dtype=torch.long)\n","        for node in anomaly_nodes:\n","            if node < G.number_of_nodes():\n","                y[node] = 1\n","    else:\n","        y = None\n","\n","    # Create PyTorch Geometric Data object\n","    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n","\n","    return data\n","\n","# Prepare financial network data\n","pyg_data = prepare_pyg_data(G_financial, anomaly_nodes)\n","print(pyg_data)\n","\n","# Check data shape\n","print(f\"Number of nodes: {pyg_data.num_nodes}\")\n","print(f\"Number of edges: {pyg_data.num_edges}\")\n","print(f\"Number of node features: {pyg_data.num_node_features}\")\n","print(f\"Number of anomalies: {pyg_data.y.sum().item()}\")\n"],"metadata":{"id":"6aVHPx4gwa9E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.3 Graph Convolutional Networks (GCNs)\n","\n","The Graph Convolutional Network is formulated as:\n","\n","$$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$\n","\n","Where:\n","- $\\tilde{A} = A + I$ is the adjacency matrix with added self-loops\n","- $\\tilde{D}$ is the degree matrix of $\\tilde{A}$\n","- $H^{(l)}$ is the node feature matrix at layer $l$\n","- $W^{(l)}$ is the learnable weight matrix\n","- $\\sigma$ is an activation function like ReLU\n","\n","Let's implement a basic GCN model for graph anomaly detection.\n"],"metadata":{"id":"bfZrPU1Rwgyc"}},{"cell_type":"code","source":["class GCN(nn.Module):\n","    \"\"\"\n","    Graph Convolutional Network for anomaly detection.\n","    \"\"\"\n","    def __init__(self, in_features, hidden_dim, out_features):\n","        \"\"\"\n","        Initialize GCN model.\n","\n","        Parameters:\n","        -----------\n","        in_features : int\n","            Number of input features\n","        hidden_dim : int\n","            Dimension of hidden layers\n","        out_features : int\n","            Number of output features/classes\n","        \"\"\"\n","        super(GCN, self).__init__()\n","\n","        # First GCN layer\n","        self.conv1 = GCNConv(in_features, hidden_dim)\n","\n","        # Second GCN layer\n","        self.conv2 = GCNConv(hidden_dim, hidden_dim // 2)\n","\n","        # Output layer\n","        self.linear = nn.Linear(hidden_dim // 2, out_features)\n","\n","        # Dropout for regularization\n","        self.dropout = nn.Dropout(0.3)\n","\n","    def forward(self, x, edge_index):\n","        \"\"\"\n","        Forward pass.\n","\n","        Parameters:\n","        -----------\n","        x : torch.Tensor\n","            Node feature matrix\n","        edge_index : torch.Tensor\n","            Graph connectivity in COO format\n","\n","        Returns:\n","        --------\n","        torch.Tensor\n","            Output predictions\n","        \"\"\"\n","        # First layer\n","        h = self.conv1(x, edge_index)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","\n","        # Second layer\n","        h = self.conv2(h, edge_index)\n","        h = F.relu(h)\n","\n","        # Output layer\n","        out = self.linear(h)\n","\n","        return out\n","\n","    def get_embeddings(self, x, edge_index):\n","        \"\"\"\n","        Get node embeddings from the model.\n","\n","        Parameters:\n","        -----------\n","        x : torch.Tensor\n","            Node feature matrix\n","        edge_index : torch.Tensor\n","            Graph connectivity in COO format\n","\n","        Returns:\n","        --------\n","        torch.Tensor\n","            Node embeddings\n","        \"\"\"\n","        # First layer\n","        h = self.conv1(x, edge_index)\n","        h = F.relu(h)\n","\n","        # Second layer\n","        h = self.conv2(h, edge_index)\n","        h = F.relu(h)\n","\n","        return h\n","\n","# Train a GCN model for anomaly detection\n","def train_eval_gcn(data, hidden_dim=64, epochs=200, lr=0.01):\n","    \"\"\"\n","    Train and evaluate a GCN model for anomaly detection.\n","\n","    Parameters:\n","    -----------\n","    data : torch_geometric.data.Data\n","        Input data\n","    hidden_dim : int\n","        Hidden dimension size\n","    epochs : int\n","        Number of training epochs\n","    lr : float\n","        Learning rate\n","\n","    Returns:\n","    --------\n","    model : GCN\n","        Trained GCN model\n","    \"\"\"\n","    # Create model\n","    model = GCN(in_features=data.num_node_features,\n","               hidden_dim=hidden_dim,\n","               out_features=2).to(device)  # 2 classes: normal and anomaly\n","\n","    # Move data to device\n","    data = data.to(device)\n","\n","    # Define optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n","\n","    # Define loss function with class weighting due to imbalance\n","    num_anomalies = data.y.sum().item()\n","    num_normal = len(data.y) - num_anomalies\n","    weight = torch.tensor([1.0/num_normal, 1.0/num_anomalies]).to(device)\n","    criterion = nn.CrossEntropyLoss(weight=weight)\n","\n","    # Training loop\n","    model.train()\n","    losses = []\n","\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        out = model(data.x, data.edge_index)\n","        loss = criterion(out, data.y)\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        losses.append(loss.item())\n","\n","        # Print progress\n","        if (epoch + 1) % 10 == 0:\n","            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n","\n","    # Plot training curve\n","    plt.figure(figsize=(10, 4))\n","    plt.plot(losses)\n","    plt.title('GCN Training Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.grid(True, alpha=0.3)\n","    plt.show()\n","\n","    # Evaluation\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data.x, data.edge_index)\n","        pred = out.argmax(dim=1)\n","\n","        # Calculate metrics\n","        correct = (pred == data.y).sum().item()\n","        accuracy = correct / len(data.y)\n","\n","        # For anomaly detection specifically\n","        true_positives = ((pred == 1) & (data.y == 1)).sum().item()\n","        false_positives = ((pred == 1) & (data.y == 0)).sum().item()\n","        false_negatives = ((pred == 0) & (data.y == 1)).sum().item()\n","\n","        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n","        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n","        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(f\"Precision: {precision:.4f}\")\n","        print(f\"Recall: {recall:.4f}\")\n","        print(f\"F1 Score: {f1:.4f}\")\n","\n","    return model\n","\n","# Train GCN model\n","gcn_model = train_eval_gcn(pyg_data, hidden_dim=64, epochs=100)\n","\n","# Get embeddings\n","with torch.no_grad():\n","    node_embeddings = gcn_model.get_embeddings(pyg_data.x, pyg_data.edge_index).cpu().numpy()\n"],"metadata":{"id":"yQAmsIYJwiv7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.4 Visualizing GCN Embeddings\n","\n","Let's visualize the node embeddings learned by our GCN model to see how well it separates normal and anomalous nodes.\n"],"metadata":{"id":"qiY6-5S7wnPt"}},{"cell_type":"code","source":["# Visualize GCN embeddings\n","def visualize_embeddings(embeddings, labels, title):\n","    \"\"\"\n","    Visualize embeddings using t-SNE.\n","\n","    Parameters:\n","    -----------\n","    embeddings : ndarray\n","        Node embeddings\n","    labels : ndarray\n","        Node labels (0 for normal, 1 for anomaly)\n","    title : str\n","        Plot title\n","    \"\"\"\n","    # Apply t-SNE\n","    tsne = TSNE(n_components=2, random_state=42)\n","    embeddings_2d = tsne.fit_transform(embeddings)\n","\n","    # Plot\n","    plt.figure(figsize=(10, 8))\n","\n","    # Plot normal nodes\n","    normal_mask = (labels == 0)\n","    plt.scatter(embeddings_2d[normal_mask, 0], embeddings_2d[normal_mask, 1],\n","               c='lightblue', s=50, label='Normal')\n","\n","    # Plot anomalous nodes\n","    anomaly_mask = (labels == 1)\n","    plt.scatter(embeddings_2d[anomaly_mask, 0], embeddings_2d[anomaly_mask, 1],\n","               c='red', s=100, label='Anomaly')\n","\n","    plt.title(title, fontsize=16)\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.show()\n","\n","# Get node labels\n","labels = pyg_data.y.cpu().numpy()\n","\n","# Visualize GCN embeddings\n","visualize_embeddings(node_embeddings, labels, 'GCN Node Embeddings')\n"],"metadata":{"id":"sqif6lPbwpPj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. GCN Autoencoders for Anomaly Detection\n","\n","Autoencoders are unsupervised learning models that learn to reconstruct their input data. When applied to graphs using GCNs, they can learn normal graph patterns and identify anomalies as instances that are difficult to reconstruct.\n","\n","### 6.1 Basic GCN Autoencoder\n","\n","A GCN autoencoder consists of:\n","- An encoder that compresses node features and graph structure into a latent space\n","- A decoder that reconstructs the original graph from the latent representation\n","\n","For anomaly detection, we train on normal data and detect nodes with high reconstruction error.\n"],"metadata":{"id":"zdpGWbdWwsLl"}},{"cell_type":"code","source":["class GCNAutoencoder(nn.Module):\n","    \"\"\"\n","    Graph Convolutional Network Autoencoder for unsupervised anomaly detection.\n","    \"\"\"\n","    def __init__(self, in_features, hidden_dim, latent_dim):\n","        \"\"\"\n","        Initialize GCN Autoencoder.\n","\n","        Parameters:\n","        -----------\n","        in_features : int\n","            Number of input features\n","        hidden_dim : int\n","            Dimension of hidden layers\n","        latent_dim : int\n","            Dimension of latent space\n","        \"\"\"\n","        super(GCNAutoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder_conv1 = GCNConv(in_features, hidden_dim)\n","        self.encoder_conv2 = GCNConv(hidden_dim, latent_dim)\n","\n","        # Decoder for feature reconstruction\n","        self.decoder_linear1 = nn.Linear(latent_dim, hidden_dim)\n","        self.decoder_linear2 = nn.Linear(hidden_dim, in_features)\n","\n","    def encode(self, x, edge_index):\n","        \"\"\"\n","        Encode node features into latent space.\n","\n","        Parameters:\n","        -----------\n","        x : torch.Tensor\n","            Node feature matrix\n","        edge_index : torch.Tensor\n","            Graph connectivity in COO format\n","\n","        Returns:\n","        --------\n","        torch.Tensor\n","            Latent representations\n","        \"\"\"\n","        h = self.encoder_conv1(x, edge_index)\n","        h = F.relu(h)\n","        h = self.encoder_conv2(h, edge_index)\n","        return h\n","\n","    def decode(self, z):\n","        \"\"\"\n","        Decode latent representations to reconstruct node features.\n","\n","        Parameters:\n","        -----------\n","        z : torch.Tensor\n","            Latent representations\n","\n","        Returns:\n","        --------\n","        torch.Tensor\n","            Reconstructed node features\n","        \"\"\"\n","        h = self.decoder_linear1(z)\n","        h = F.relu(h)\n","        return self.decoder_linear2(h)\n","\n","    def decode_structure(self, z):\n","        \"\"\"\n","        Decode latent representations to reconstruct adjacency matrix.\n","\n","        Parameters:\n","        -----------\n","        z : torch.Tensor\n","            Latent representations\n","\n","        Returns:\n","        --------\n","        torch.Tensor\n","            Reconstructed adjacency matrix\n","        \"\"\"\n","        # Inner product decoder: A' = σ(ZZ^T)\n","        adj_pred = torch.sigmoid(torch.matmul(z, z.t()))\n","        return adj_pred\n","\n","    def forward(self, x, edge_index):\n","        \"\"\"\n","        Forward pass.\n","\n","        Parameters:\n","        -----------\n","        x : torch.Tensor\n","            Node feature matrix\n","        edge_index : torch.Tensor\n","            Graph connectivity in COO format\n","\n","        Returns:\n","        --------\n","        tuple\n","            (encoded, reconstructed_features, reconstructed_adjacency)\n","        \"\"\"\n","        # Encode\n","        z = self.encode(x, edge_index)\n","\n","        # Decode features\n","        x_hat = self.decode(z)\n","\n","        # Decode structure\n","        adj_hat = self.decode_structure(z)\n","\n","        return z, x_hat, adj_hat\n","\n","# Train the GCN Autoencoder\n","def train_gcn_autoencoder(data, hidden_dim=32, latent_dim=16, epochs=200, lr=0.01,\n","                         alpha=0.5, use_unsupervised=True):\n","    \"\"\"\n","    Train a GCN Autoencoder for anomaly detection.\n","\n","    Parameters:\n","    -----------\n","    data : torch_geometric.data.Data\n","        Input data\n","    hidden_dim : int\n","        Hidden dimension size\n","    latent_dim : int\n","        Latent dimension size\n","    epochs : int\n","        Number of training epochs\n","    lr : float\n","        Learning rate\n","    alpha : float\n","        Weight for structure vs. feature reconstruction\n","    use_unsupervised : bool\n","        If True, train only on normal nodes. If False, use all nodes.\n","\n","    Returns:\n","    --------\n","    model : GCNAutoencoder\n","        Trained model\n","    \"\"\"\n","    # Create model\n","    model = GCNAutoencoder(in_features=data.num_node_features,\n","                          hidden_dim=hidden_dim,\n","                          latent_dim=latent_dim).to(device)\n","\n","    # Create adjacency matrix for reconstruction target\n","    edge_index = data.edge_index.cpu().numpy()\n","    num_nodes = data.num_nodes\n","    adj = torch.zeros((num_nodes, num_nodes), device=device)\n","    adj[edge_index[0], edge_index[1]] = 1\n","\n","    # Move data to device\n","    data = data.to(device)\n","\n","    # Select training nodes (only normal nodes if use_unsupervised=True)\n","    if use_unsupervised:\n","        train_mask = (data.y == 0)\n","    else:\n","        train_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n","\n","    # Optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    # Training loop\n","    model.train()\n","    losses = []\n","\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        z, x_hat, adj_hat = model(data.x, data.edge_index)\n","\n","        # Feature reconstruction loss\n","        feat_loss = F.mse_loss(x_hat[train_mask], data.x[train_mask])\n","\n","        # Structure reconstruction loss (only for connections involving training nodes)\n","        struct_loss = F.binary_cross_entropy(adj_hat[train_mask][:, train_mask],\n","                                           adj[train_mask][:, train_mask])\n","\n","        # Combined loss\n","        loss = alpha * struct_loss + (1 - alpha) * feat_loss\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        losses.append(loss.item())\n","\n","        # Print progress\n","        if (epoch + 1) % 10 == 0:\n","            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}')\n","\n","    # Plot training curve\n","    plt.figure(figsize=(10, 4))\n","    plt.plot(losses)\n","    plt.title('GCN Autoencoder Training Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.grid(True, alpha=0.3)\n","    plt.show()\n","\n","    return model\n","\n","# Train GCN autoencoder\n","gcn_ae_model = train_gcn_autoencoder(pyg_data, hidden_dim=32, latent_dim=16,\n","                                    epochs=100, alpha=0.5, use_unsupervised=True)\n"],"metadata":{"id":"5I-O0TaTwuTk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.2 Using GCN Autoencoder for Anomaly Detection\n","\n","Now that we've trained our autoencoder, let's use it to detect anomalies by calculating reconstruction errors.\n"],"metadata":{"id":"PU7Z7Vdswybl"}},{"cell_type":"code","source":["# Calculate reconstruction errors for anomaly detection\n","def gcn_ae_anomaly_detection(model, data, alpha=0.5, threshold_percentile=95):\n","    \"\"\"\n","    Detect anomalies using GCN autoencoder reconstruction errors.\n","\n","    Parameters:\n","    -----------\n","    model : GCNAutoencoder\n","        Trained autoencoder model\n","    data : torch_geometric.data.Data\n","        Input data\n","    alpha : float\n","        Weight for structure vs. feature reconstruction error\n","    threshold_percentile : float\n","        Percentile for threshold calculation\n","\n","    Returns:\n","    --------\n","    anomalies : list\n","        Detected anomalous nodes\n","    scores : array\n","        Anomaly scores for all nodes\n","    \"\"\"\n","    model.eval()\n","\n","    # Create adjacency matrix\n","    edge_index = data.edge_index.cpu().numpy()\n","    num_nodes = data.num_nodes\n","    adj = torch.zeros((num_nodes, num_nodes), device=device)\n","    adj[edge_index[0], edge_index[1]] = 1\n","\n","    # Forward pass\n","    with torch.no_grad():\n","        z, x_hat, adj_hat = model(data.x.to(device), data.edge_index.to(device))\n","\n","        # Calculate feature reconstruction error per node\n","        feat_errors = F.mse_loss(x_hat, data.x.to(device), reduction='none')\n","        feat_errors = feat_errors.mean(dim=1).cpu().numpy()\n","\n","        # Calculate structure reconstruction error per node\n","        struct_errors = F.binary_cross_entropy(adj_hat, adj, reduction='none')\n","        struct_errors = struct_errors.mean(dim=1).cpu().numpy()\n","\n","        # Combined error\n","        errors = alpha * struct_errors + (1 - alpha) * feat_errors\n","\n","        # Determine threshold from normal nodes\n","        normal_mask = (data.y.cpu().numpy() == 0)\n","        normal_errors = errors[normal_mask]\n","        threshold = np.percentile(normal_errors, threshold_percentile)\n","\n","        # Detect anomalies\n","        anomalies = np.where(errors > threshold)[0]\n","\n","    return anomalies, errors, threshold\n","\n","# Detect anomalies using GCN autoencoder\n","ae_anomalies, ae_scores, ae_threshold = gcn_ae_anomaly_detection(gcn_ae_model, pyg_data)\n","\n","# Evaluate results\n","print(\"\\nGCN Autoencoder Results:\")\n","print(f\"True anomalies: {len(anomaly_nodes)}\")\n","print(f\"Detected anomalies: {len(ae_anomalies)}\")\n","print(\"\\nPerformance:\")\n","ae_performance = evaluate_anomaly_detection(ae_anomalies, anomaly_nodes)\n","\n","# Visualize autoencoder latent space\n","with torch.no_grad():\n","    z, _, _ = gcn_ae_model(pyg_data.x.to(device), pyg_data.edge_index.to(device))\n","    latent_space = z.cpu().numpy()\n","\n","# Visualize latent embeddings\n","visualize_embeddings(latent_space, pyg_data.y.cpu().numpy(), 'GCN Autoencoder Latent Space')\n","\n","# Visualize reconstruction errors\n","plt.figure(figsize=(12, 6))\n","\n","# Sort nodes by error score\n","sorted_indices = np.argsort(ae_scores)[::-1]  # Descending order\n","sorted_scores = ae_scores[sorted_indices]\n","sorted_labels = pyg_data.y.cpu().numpy()[sorted_indices]\n","\n","# Bar colors\n","colors = ['red' if label == 1 else 'lightblue' for label in sorted_labels]\n","\n","# Plot top 50 nodes by error\n","plt.bar(range(50), sorted_scores[:50], color=colors[:50])\n","plt.axhline(y=ae_threshold, color='red', linestyle='--', label='Threshold')\n","plt.title('Top 50 Nodes by Reconstruction Error', fontsize=16)\n","plt.xlabel('Node Rank')\n","plt.ylabel('Reconstruction Error')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","plt.show()\n"],"metadata":{"id":"sjJTqNwKw0Vk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Comparing Methods and Visualizing Results\n","\n","Let's compile the results from all the methods we've implemented and visualize their performance.\n"],"metadata":{"id":"v0niQD7cw3ZZ"}},{"cell_type":"code","source":["# Compile results from all methods\n","methods = ['Statistical', 'Community-Based', 'Spectral', 'Node2Vec', 'GCN', 'GCN Autoencoder']\n","\n","# Collect performance metrics\n","precisions = [\n","    stat_performance[0],\n","    comm_performance[0],\n","    spec_performance[0],\n","    embedding_results['Balanced']['performance'][0],\n","    ae_performance[0]  # GCN Autoencoder\n","]\n","\n","recalls = [\n","    stat_performance[1],\n","    comm_performance[1],\n","    spec_performance[1],\n","    embedding_results['Balanced']['performance'][1],\n","    ae_performance[1]  # GCN Autoencoder\n","]\n","\n","f1_scores = [\n","    stat_performance[2],\n","    comm_performance[2],\n","    spec_performance[2],\n","    embedding_results['Balanced']['performance'][2],\n","    ae_performance[2]  # GCN Autoencoder\n","]\n","\n","# Create comparison plot\n","plt.figure(figsize=(12, 6))\n","x = np.arange(len(methods) - 1)  # Skip GCN supervised as it's not directly comparable\n","width = 0.25\n","\n","plt.bar(x - width, precisions, width, label='Precision', color='skyblue')\n","plt.bar(x, recalls, width, label='Recall', color='lightgreen')\n","plt.bar(x + width, f1_scores, width, label='F1 Score', color='salmon')\n","\n","plt.xlabel('Method', fontsize=14)\n","plt.ylabel('Score', fontsize=14)\n","plt.title('Comparison of Anomaly Detection Methods', fontsize=16)\n","plt.xticks(x, methods[:-1])\n","plt.ylim(0, 1)\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n","# Visualize node2vec vs GCN autoencoder embeddings\n","plt.figure(figsize=(16, 6))\n","\n","# Node2Vec embeddings\n","plt.subplot(1, 2, 1)\n","nodes = list(embedding_results['Balanced']['embeddings'].keys())\n","X = np.array([embedding_results['Balanced']['embeddings'][node] for node in nodes])\n","X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\n","\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1],\n","          c=[pyg_data.y[i].item() for i in range(len(nodes))],\n","          cmap='coolwarm', s=50)\n","plt.title('Node2Vec Embeddings', fontsize=16)\n","plt.colorbar(label='Is Anomaly')\n","\n","# GCN Autoencoder embeddings\n","plt.subplot(1, 2, 2)\n","plt.scatter(latent_space[:, 0], latent_space[:, 1],\n","          c=pyg_data.y.cpu().numpy(), cmap='coolwarm', s=50)\n","plt.title('GCN Autoencoder Latent Space (2D Projection)', fontsize=16)\n","plt.colorbar(label='Is Anomaly')\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"nRAW66vWw5lq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. Conclusion and Key Takeaways\n","\n","In this notebook, we've explored a wide range of graph-based anomaly detection methods, from simple statistical approaches to advanced deep learning models.\n","\n","### Key Points:\n","\n","1. **Graph Structure Matters**: Anomalies in graphs often manifest as unusual connectivity patterns or structural properties that can't be detected by looking at nodes in isolation.\n","\n","2. **Node Features Enhance Detection**: Combining structural information with node features can significantly improve anomaly detection, especially for complex patterns.\n","\n","3. **Method Selection Depends on Context**:\n","   - **Statistical methods**: Fast and interpretable, good for simple anomalies\n","   - **Embedding methods**: Balance of efficiency and effectiveness\n","   - **GNN-based approaches**: Best for complex, subtle anomalies\n","\n","4. **Advantages of Graph Neural Networks**:\n","   - Learn representations that capture both structure and features\n","   - Can generalize to unseen nodes and graphs\n","   - Detect complex anomaly patterns\n","\n","5. **Unsupervised Learning is Powerful**: Autoencoder-based approaches can detect anomalies without labeled examples, making them particularly useful in real-world scenarios.\n","\n","### Applications:\n","\n","These techniques have applications across various domains:\n","- Financial fraud detection\n","- Network security and intrusion detection\n","- Social network analysis\n","- Biological network analysis\n","- Supply chain monitoring\n","- Infrastructure fault detection\n","\n","### Future Directions:\n","- **Temporal graph neural networks** for evolving graphs\n","- **Self-supervised learning** approaches to better leverage unlabeled data\n","- **Explainable AI** methods to interpret anomaly detection results\n"],"metadata":{"id":"kmujgJafw8qj"}}]}