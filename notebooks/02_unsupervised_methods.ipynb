{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871b85b6",
   "metadata": {},
   "source": [
    "# Notebookâ€¯2 â€“ Unsupervised Anomaly Detection Methods  \n",
    "*Generated 2025-05-13*\n",
    "\n",
    "This notebook aligns with **Slidesâ€¯21â€‘40**, diving deep into three unsupervised detectors:\n",
    "Isolation Forest, Oneâ€‘ClassÂ SVM, and Autoencoders.\n",
    "\n",
    "> Work in driverâ€‘navigator pairs; swap roles at each ðŸ›‘ checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df33a5b",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Dataset for Experiments\n",
    "\n",
    "To avoid overfitting to the creditâ€‘card data, we use the **Breast Cancer Wisconsin Diagnostic** dataset from scikitâ€‘learn.\n",
    "\n",
    "*We will **treat the malignant cases as â€œanomaliesâ€** (â‰ˆ37â€¯% of data). Not a perfect realâ€‘world ratio, but sufficient for comparison.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, f1_score\n",
    "\n",
    "# Load\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "y = pd.Series(data['target'])  # 0 = malignant (anomaly), 1 = benign (normal)\n",
    "\n",
    "print(\"Benign (normal):\", (y==1).sum(), \"| Malignant (anomaly):\", (y==0).sum())\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c3cf5",
   "metadata": {},
   "source": [
    "### ðŸ” Class distribution bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bar chart benign vs malignant\n",
    "counts = y.value_counts()\n",
    "plt.figure()\n",
    "plt.bar(['Benign (normal)', 'Malignant (anomaly)'], counts.values)\n",
    "plt.title(\"Dataset Class Distribution\")\n",
    "plt.ylabel(\"Count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b2577",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Isolation Forest (IF)\n",
    "\n",
    "**Core principle**: Anomalies are easier to **isolate** under random axisâ€‘parallel splits.\n",
    "\n",
    "Algorithm walkthrough  \n",
    "1. Build many binary trees using random subâ€‘sample.  \n",
    "2. Each split randomly selects a feature and a split value within its range.  \n",
    "3. **Path length** from root to leaf â‰ˆ how many splits required to isolate a point.  \n",
    "4. Average path length across trees â†’ anomaly score (shorter âžœ more anomalous).\n",
    "\n",
    "**Ensemble strength**: aggregation reduces variance; logarithmic pathâ€‘length reference normalises score.\n",
    "\n",
    "**Important hyperâ€‘parameters**\n",
    "\n",
    "| Param | Effect |\n",
    "|-------|--------|\n",
    "| `n_estimators` | # trees (â‰¥100 typical) |\n",
    "| `max_samples` | subsample size per tree |\n",
    "| `contamination` | expected anomaly proportion (affects threshold) |\n",
    "| `max_features` | # features per split |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Fit on training data (treating all as unlabeled; contamination estimate 0.35)\n",
    "iso = IsolationForest(n_estimators=200,\n",
    "                      contamination=0.35,\n",
    "                      random_state=42)\n",
    "iso.fit(X_train)\n",
    "\n",
    "# decision_function returns + (normal) to - (anomaly); invert for anomaly score\n",
    "iso_score = -iso.decision_function(X_test)\n",
    "\n",
    "# Metrics\n",
    "precision_if, recall_if, _ = precision_recall_curve((y_test==0).astype(int), iso_score)\n",
    "pr_auc_if = auc(recall_if, precision_if)\n",
    "fpr_if, tpr_if, _ = roc_curve((y_test==0).astype(int), iso_score)\n",
    "roc_auc_if = auc(fpr_if, tpr_if)\n",
    "\n",
    "print(f\"Isolation Forest  PRâ€‘AUC={pr_auc_if:.3f}  ROCâ€‘AUC={roc_auc_if:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e931c6",
   "metadata": {},
   "source": [
    "#### ðŸŽ² Random split illustration (2â€‘D toy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d2a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Synthetic 2D toy dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "X_toy, _ = make_blobs(n_samples=200, centers=[[0,0]], cluster_std=1.2, random_state=0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_toy[:,0], X_toy[:,1], s=20, alpha=0.4)\n",
    "\n",
    "# Draw 3 random axis-parallel splits\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ymin, ymax = ax.get_ylim()\n",
    "for _ in range(3):\n",
    "    if np.random.rand() > 0.5:\n",
    "        x_split = np.random.uniform(xmin, xmax)\n",
    "        ax.axvline(x_split, color='red', linestyle='--')\n",
    "    else:\n",
    "        y_split = np.random.uniform(ymin, ymax)\n",
    "        ax.axhline(y_split, color='blue', linestyle='--')\n",
    "\n",
    "ax.set_title(\"Random axisâ€‘parallel splits â€“ one tree level\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1788b",
   "metadata": {},
   "source": [
    "#### ðŸŒ² Miniâ€‘forest structure (3 iTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Graphical mini Isol. Forest\n",
    "import networkx as nx\n",
    "def build_random_tree(depth=3):\n",
    "    G = nx.DiGraph()\n",
    "    def add_nodes(parent, depth_left):\n",
    "        if depth_left == 0:\n",
    "            return\n",
    "        left = parent + \"L\"\n",
    "        right = parent + \"R\"\n",
    "        G.add_edge(parent, left)\n",
    "        G.add_edge(parent, right)\n",
    "        add_nodes(left, depth_left-1)\n",
    "        add_nodes(right, depth_left-1)\n",
    "    G.add_node(\"root\")\n",
    "    add_nodes(\"root\", depth)\n",
    "    return G\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(12,3))\n",
    "for i in range(3):\n",
    "    G = build_random_tree(3)\n",
    "    pos = nx.drawing.nx_agraph.graphviz_layout(G, prog='dot')\n",
    "    nx.draw(G, pos, ax=axes[i], node_size=150, arrows=False)\n",
    "    axes[i].set_title(f\"Tree {i+1}\")\n",
    "    axes[i].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225912cf",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Oneâ€‘Class SVM (OCâ€‘SVM)\n",
    "\n",
    "SVM seeks the **maxâ€‘margin hyperplane** separating classes.  \n",
    "In the **oneâ€‘class** setting, we have only *normal* data; OCâ€‘SVM learns a **boundary** tightly enclosing them.\n",
    "\n",
    "*Kernel trick*: map data into higherâ€‘D feature space to separate â€œnormalâ€ from origin.\n",
    "\n",
    "**Key hyperâ€‘parameters**\n",
    "\n",
    "| Param | Interpretation |\n",
    "|-------|----------------|\n",
    "| `Î½` (nu) | Upper bound on training outlier fraction and lower bound on support vectors |\n",
    "| `Î³` | RBF kernel width; small Î³ âžœ smoother boundary |\n",
    "| `kernel` | Usually `\"rbf\"` for nonâ€‘linear boundary |\n",
    "\n",
    "**Strengths**\n",
    "\n",
    "* Captures complex boundaries via kernels.  \n",
    "* Theoretically grounded.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Poor scaling to >50â€¯k samples.  \n",
    "* Sensitive to Î³ & Î½ tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da01ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Train only on normal instances (label==1)\n",
    "X_train_norm = X_train[y_train == 1]\n",
    "\n",
    "ocsvm = OneClassSVM(kernel='rbf', nu=0.05, gamma='scale')\n",
    "ocsvm.fit(X_train_norm)\n",
    "\n",
    "svm_score = -ocsvm.score_samples(X_test)  # higher = more anomalous\n",
    "precision_svm, recall_svm, _ = precision_recall_curve((y_test==0).astype(int), svm_score)\n",
    "pr_auc_svm = auc(recall_svm, precision_svm)\n",
    "fpr_svm, tpr_svm, _ = roc_curve((y_test==0).astype(int), svm_score)\n",
    "roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "print(f\"OCâ€‘SVM  PRâ€‘AUC={pr_auc_svm:.3f}  ROCâ€‘AUC={roc_auc_svm:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc51016",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Autoencoders for Anomaly Detection\n",
    "\n",
    "An **Autoencoder (AE)** is a neural network trained to **reconstruct** its input via a compressed **bottleneck**.\n",
    "\n",
    "*Idea*: Train AE on **normal** data â†’ low reconstruction error for normal points, **high error for anomalies**.\n",
    "\n",
    "Workflow  \n",
    "1. Train AE on normal subset.  \n",
    "2. Score = reconstruction meanâ€‘squaredâ€‘error.  \n",
    "3. Threshold high MSE as anomalies.\n",
    "\n",
    "**Hyperâ€‘parameters**\n",
    "\n",
    "| Hyperâ€‘param | Effect |\n",
    "|-------------|--------|\n",
    "| Latent dimension | Smaller â†’ more compression, higher contrast |\n",
    "| Architecture depth | Capacity to represent normal data |\n",
    "| Epochs/batch_size | Training convergence |\n",
    "| Reconstruction loss | Usually MSE for numeric |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8bfdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Build small dense AE\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 10\n",
    "\n",
    "inp = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(32, activation='relu')(inp)\n",
    "bottleneck = layers.Dense(encoding_dim, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(bottleneck)\n",
    "out = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "ae = models.Model(inp, out)\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "early = callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train on normal\n",
    "ae.fit(X_train_norm, X_train_norm,\n",
    "       epochs=20, batch_size=128,\n",
    "       validation_split=0.1, callbacks=[early], verbose=0)\n",
    "\n",
    "# Score by reconstruction error\n",
    "recon = ae.predict(X_test, batch_size=256)\n",
    "mse = np.mean(np.square(recon - X_test), axis=1)\n",
    "\n",
    "precision_ae, recall_ae, _ = precision_recall_curve((y_test==0).astype(int), mse)\n",
    "pr_auc_ae = auc(recall_ae, precision_ae)\n",
    "fpr_ae, tpr_ae, _ = roc_curve((y_test==0).astype(int), mse)\n",
    "roc_auc_ae = auc(fpr_ae, tpr_ae)\n",
    "\n",
    "print(f\"Autoencoder  PRâ€‘AUC={pr_auc_ae:.3f}  ROCâ€‘AUC={roc_auc_ae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d03d8",
   "metadata": {},
   "source": [
    "#### ðŸ–¼ï¸ Autoencoder architecture sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple AE architecture diagram\n",
    "layers_labels = [\"Input (30)\", \"Dense 32\", \"Latent 10\", \"Dense 32\", \"Output (30)\"]\n",
    "y_positions = list(range(len(layers_labels)))[::-1]\n",
    "plt.figure(figsize=(4,3))\n",
    "for y, label in zip(y_positions, layers_labels):\n",
    "    plt.text(0.5, y, label, ha='center', va='center',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"lightblue\"))\n",
    "for y in y_positions[:-1]:\n",
    "    plt.plot([0.5,0.5], [y-0.4, y-0.6], color='gray')\n",
    "plt.ylim(-1, len(layers_labels))\n",
    "plt.axis('off')\n",
    "plt.title(\"AE network schematic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Individual PR curves\n",
    "plt.figure()\n",
    "plt.plot(recall_if, precision_if, label=\"IF\")\n",
    "plt.plot(recall_svm, precision_svm, label=\"OCâ€‘SVM\")\n",
    "plt.plot(recall_ae, precision_ae, label=\"AE\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precisionâ€‘Recall curves\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Individual ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr_if, tpr_if, label=\"IF\")\n",
    "plt.plot(fpr_svm, tpr_svm, label=\"OCâ€‘SVM\")\n",
    "plt.plot(fpr_ae, tpr_ae, label=\"AE\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curves\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1aff8",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Evaluation Metrics & Threshold Strategies\n",
    "\n",
    "| Metric | Formula | When to prefer |\n",
    "|--------|---------|----------------|\n",
    "| **Precision** | TP / (TP + FP) | High cost for false positives |\n",
    "| **Recall** | TP / (TP + FN) | High cost for missed anomalies |\n",
    "| **F1** | Harmonic mean of Precision & Recall | Balanced importance |\n",
    "| **ROCâ€‘AUC** | Area under ROC (TPR vs FPR) curve | Class balance not extreme |\n",
    "| **PRâ€‘AUC** | Area under Precisionâ€‘Recall curve | Strong class imbalance |\n",
    "\n",
    "**Threshold selection**\n",
    "\n",
    "* Percentile of score on training data.  \n",
    "* Maximise F1 on a validation set.  \n",
    "* Minimize *cost* via domain weights.\n",
    "\n",
    "> ðŸ§® **Metricâ€‘optimisation exercise**: Experiment with different `contamination` for IF and observe Precisionâ€‘Recall tradeâ€‘off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168318b3",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Method Comparison â€“ PR & ROC Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PR curve overlay\n",
    "plt.figure()\n",
    "plt.plot(recall_if, precision_if, label=f\"IF (AUC={pr_auc_if:.3f})\")\n",
    "plt.plot(recall_svm, precision_svm, label=f\"OCâ€‘SVM (AUC={pr_auc_svm:.3f})\")\n",
    "plt.plot(recall_ae, precision_ae, label=f\"AE (AUC={pr_auc_ae:.3f})\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR comparison\"); plt.legend()\n",
    "\n",
    "# ROC curve overlay\n",
    "plt.figure()\n",
    "plt.plot(fpr_if, tpr_if, label=f\"IF (AUC={roc_auc_if:.3f})\")\n",
    "plt.plot(fpr_svm, tpr_svm, label=f\"OCâ€‘SVM (AUC={roc_auc_svm:.3f})\")\n",
    "plt.plot(fpr_ae, tpr_ae, label=f\"AE (AUC={roc_auc_ae:.3f})\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC comparison\"); plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b8f45",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Knowledge Check\n",
    "\n",
    "1. **Conceptual** â€“ Why might Isolation Forest outperform OCâ€‘SVM on highâ€‘dimensional sparse data?  \n",
    "2. **Hyperâ€‘parameters** â€“ What practical tradeâ€‘off does `Î½` control in OCâ€‘SVM?  \n",
    "3. **Autoencoder** â€“ If the reconstruction error for normal validation data keeps decreasing but so does for anomalies, what could be happening? Suggest a mitigation.  \n",
    "4. **Metrics** â€“ Give one scenario where ROCâ€‘AUC can be misleading compared to PRâ€‘AUC.  \n",
    "5. **Critical thinking** â€“ Based on your results, which method would you deploy for medical anomaly detection where false negatives carry extremely high cost? Why?\n",
    "\n",
    "> *Document responses in reflection sheet or discuss with peers.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fbb3af",
   "metadata": {},
   "source": [
    "### ðŸ“Š Interpreting the comparison plots\n",
    "* **Precisionâ€‘Recall** â€“ Higher curve and greater area indicate better performance under class imbalance.  \n",
    "  *If you observe IF dominating earlyâ€‘recall region but AE overtakes at high recall, it suggests IF produces highly precise topâ€‘alerts while AE recovers more anomalies overall.*\n",
    "\n",
    "* **ROC** â€“ With moderately imbalanced classes the ROC curve still helps gauge overall separability.  AUCs close to 1.0 show all three models discriminate well, but look for differences in the **lowâ€‘FPR** region (leftâ€‘hand side) â€“ critical in highâ€‘cost domains.\n",
    "\n",
    "* **Takeâ€‘away for this dataset**  \n",
    "  * Isolationâ€¯Forest typically has the best early precision (useful for triage).  \n",
    "  * Autoencoder may edge ahead in ROCâ€‘AUC thanks to nuanced reconstruction error.  \n",
    "  * OCâ€‘SVM lags slightly, highlighting its sensitivity to Î½/Î³ tuning and smallâ€‘sample training.\n",
    "\n",
    "Tie these visual insights back to your deployment context: choose the metric that aligns with business cost (missed anomaly vs investigation effort).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
