{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245f8359",
   "metadata": {},
   "source": [
    "# Notebook‚ÄØ3 ‚Äì Explainability in Anomaly Detection  \n",
    "*Generated 2025-05-13*\n",
    "\n",
    "This notebook complements **Slides‚ÄØ41‚Äë55**, focusing on why & how to explain anomaly‚Äëdetection models using **SHAP**.\n",
    "\n",
    "> Goal: move beyond *black‚Äëbox* scores to interpretable, actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796709f9",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Why Explainability?\n",
    "\n",
    "| Motivation | Example |\n",
    "|------------|---------|\n",
    "| **Trust** | Security analyst accepts alert if rationale makes sense. |\n",
    "| **Actionability** | Pinpoint which feature (e.g., IP range) to block. |\n",
    "| **Model improvement** | Spot spurious correlations driving false positives. |\n",
    "| **Regulatory** | GDPR ‚Äúright to explanation‚Äù for automated decisions. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478187a0",
   "metadata": {},
   "source": [
    "> **Colab Tip**: First run `!pip install shap -q` if SHAP is not pre‚Äëinstalled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, shap, matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import warnings, os\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f665289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî® Create synthetic 'login attempt' dataset\n",
    "np.random.seed(42)\n",
    "n_normal = 500\n",
    "n_anom = 25\n",
    "\n",
    "normal = pd.DataFrame({\n",
    "    \"failed_logins_last_5min\": np.random.poisson(2, n_normal),\n",
    "    \"hour_of_day\": np.random.randint(0, 24, n_normal),\n",
    "    \"geo_distance_km\": np.random.gamma(2, 20, n_normal)  # distance from usual location\n",
    "})\n",
    "\n",
    "anom = pd.DataFrame({\n",
    "    \"failed_logins_last_5min\": np.random.poisson(15, n_anom),\n",
    "    \"hour_of_day\": np.random.randint(0, 24, n_anom),\n",
    "    \"geo_distance_km\": np.random.gamma(2, 50, n_anom) + 200\n",
    "})\n",
    "\n",
    "df = pd.concat([normal, anom], ignore_index=True)\n",
    "y = pd.Series([0]*n_normal + [1]*n_anom)  # 1 = anomaly\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e473864",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ SHAP Basics\n",
    "\n",
    "**SHapley Additive exPlanations** decompose a prediction into feature contributions respecting game‚Äëtheoretic fairness.\n",
    "\n",
    "Visuals:\n",
    "* **Bar plot** ‚Äì mean absolute SHAP value per feature (global importance).  \n",
    "* **Force plot** ‚Äì per‚Äëinstance explanation (pushes ‚Üë prediction).\n",
    "\n",
    "> SHAP value sign indicates direction: positive value pushes *towards anomaly* (higher score here).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a88cd99",
   "metadata": {},
   "source": [
    "### 2a. SHAP for Isolation Forest\n",
    "\n",
    "Tree‚Äëbased models use **`shap.TreeExplainer`** ‚Äì fast, exact path‚Äëbased calculation.\n",
    "\n",
    "Mechanism  \n",
    "1. Compute expected model output.  \n",
    "2. Evaluate contribution of each feature to deviation from expectation.\n",
    "\n",
    "Interpretation: High positive SHAP for `failed_logins_last_5min` means that spike drove the anomaly score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790846a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Isolation Forest\n",
    "iso = IsolationForest(n_estimators=200, contamination=0.05, random_state=0)\n",
    "iso.fit(X_scaled)\n",
    "\n",
    "# Use TreeExplainer\n",
    "explainer_if = shap.TreeExplainer(iso)\n",
    "shap_values_if = explainer_if.shap_values(X_scaled)\n",
    "\n",
    "# Global bar plot\n",
    "shap.summary_plot(shap_values_if, df, plot_type=\"bar\", show=False)\n",
    "plt.title(\"IF ‚Äì Global feature importance\")\n",
    "\n",
    "# Explain a single suspicious record (index -1, an anomaly)\n",
    "idx = len(df)-1\n",
    "print(\"Instance:\", df.iloc[idx].to_dict())\n",
    "shap.force_plot(explainer_if.expected_value, shap_values_if[idx], df.iloc[idx], matplotlib=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db3f92",
   "metadata": {},
   "source": [
    "### üìò Interpretation Guide ‚Äì SHAP for Isolation Forest\n",
    "\n",
    "**Global bar plot**\n",
    "\n",
    "* **Ordering** ‚Äì Longer bars = higher *mean absolute* impact on anomaly score.\n",
    "* **Units** ‚Äì Same scale as the raw IF `decision_function`; positive value means *toward normal*, negative means *toward anomaly*.\n",
    "* **Take‚Äëaway** ‚Äì Top‚Äëranked feature(s) are the primary drivers of anomalies across the whole dataset.\n",
    "\n",
    "**Per‚Äëinstance force plot**\n",
    "\n",
    "* **Grey base value** ‚Äì Average IF score on the background sample (‚âà ‚Äútypical normal¬†point‚Äù).\n",
    "* **Red arrows** ‚Äì Features *lowering* the score below the threshold ‚áí make the point look anomalous.\n",
    "* **Blue arrows** ‚Äì Features pushing the score up toward normal.\n",
    "* **Additivity check** ‚Äì Sum of all arrows + base value = final model score (Shapley property).\n",
    "\n",
    "> **Insight for analysts**: Large red arrow on `failed_logins_last_5min` pinpoints excessive login failures as the actionable root‚Äëcause of the alert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52081ffe",
   "metadata": {},
   "source": [
    "### 2b. SHAP for Autoencoder\n",
    "\n",
    "Autoencoders aren‚Äôt tree‚Äëbased; we approximate explanations with **`KernelExplainer`** on the *reconstruction error* model.\n",
    "\n",
    "Steps  \n",
    "1. Wrap the AE to output reconstruction error per sample.  \n",
    "2. Use a sample of background data (normal points) for Shapley baseline.  \n",
    "3. Interpret SHAP values: features contributing most to *high* error caused the anomaly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tiny autoencoder on normal only\n",
    "X_norm = X_scaled[y == 0]\n",
    "inp = layers.Input(shape=(X_norm.shape[1],))\n",
    "encoded = layers.Dense(10, activation='relu')(inp)\n",
    "decoded = layers.Dense(X_norm.shape[1], activation='linear')(encoded)\n",
    "ae = models.Model(inp, decoded)\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "ae.fit(X_norm, X_norm, epochs=15, batch_size=64, verbose=0)\n",
    "\n",
    "# Wrapper: output MSE scalar\n",
    "import tensorflow as tf\n",
    "def mse_model(x):\n",
    "    recon = ae.predict(x, verbose=0)\n",
    "    return np.mean(np.square(recon - x), axis=1)\n",
    "\n",
    "# KernelExplainer (may be slow; use 100 background samples)\n",
    "background = X_norm[np.random.choice(len(X_norm), 100, replace=False)]\n",
    "explainer_ae = shap.KernelExplainer(mse_model, background)\n",
    "# Explain one anomaly instance\n",
    "sample = X_scaled[-1:].copy()\n",
    "shap_values_ae = explainer_ae.shap_values(sample, nsamples=200)\n",
    "\n",
    "# Visualise\n",
    "shap.force_plot(explainer_ae.expected_value, shap_values_ae[0], df.iloc[[-1]], matplotlib=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7971ef",
   "metadata": {},
   "source": [
    "### üìó Interpretation Guide ‚Äì SHAP for Autoencoder\n",
    "\n",
    "**Why different?**  We explain the **reconstruction MSE** (higher ‚áí more anomalous).\n",
    "\n",
    "* **Red arrows** ‚Äì Features **increasing** MSE (push toward anomaly).\n",
    "* **Blue arrows** ‚Äì Features decreasing MSE.\n",
    "\n",
    "**Caveats**\n",
    "\n",
    "1. **Approximate** ‚Äì KernelExplainer samples the feature space; small variations are expected.\n",
    "2. **Scaling matters** ‚Äì Ensure inputs are standardised; otherwise large‚Äëscale features dominate SHAP magnitude.\n",
    "3. **High‚Äëdimensional data** ‚Äì SHAP values may become diffuse. Consider PCA or local masking to focus on top‚Äëk features.\n",
    "\n",
    "> **Practical reading**: A dominant red arrow on `geo_distance_km` reveals the AE had never seen such a distance during training, hence cannot reconstruct it ‚Äì that deviation generates high error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5aa2ac",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Knowledge Check\n",
    "\n",
    "1. **Trust & actionability** ‚Äì How does a SHAP force plot help a security analyst decide next steps?  \n",
    "2. **TreeExplainer vs KernelExplainer** ‚Äì When would you prefer KernelExplainer even for a tree model?  \n",
    "3. **Autoencoder explainability** ‚Äì Why might SHAP values on raw reconstruction error be misleading in high‚Äëdimensional data? Suggest a fix.  \n",
    "4. **Regulatory** ‚Äì Outline a GDPR‚Äëcompliant explanation you could generate for an Isolation Forest‚Äôs fraud flag.  \n",
    "5. **Model improvement** ‚Äì If SHAP reveals `hour_of_day` dominates anomaly scores but domain experts disagree, what steps would you take?\n",
    "\n",
    "Discuss and record answers in your reflection sheet.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
