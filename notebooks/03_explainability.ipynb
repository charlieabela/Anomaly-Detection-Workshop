{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "245f8359",
      "metadata": {
        "id": "245f8359"
      },
      "source": [
        "# Notebook 3 – Explainability in Anomaly Detection  \n",
        "*Generated 2025-05-13*\n",
        "\n",
        "This notebook complements **Slides 28‑33**, focusing on why & how to explain anomaly‑detection models using **SHAP**.\n",
        "\n",
        "> Goal: move beyond *black‑box* scores to interpretable, actionable insights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796709f9",
      "metadata": {
        "id": "796709f9"
      },
      "source": [
        "## 1️⃣ Why Explainability?\n",
        "\n",
        "| Motivation | Example |\n",
        "|------------|---------|\n",
        "| **Trust** | Security analyst accepts alert if rationale makes sense. |\n",
        "| **Actionability** | Pinpoint which feature (e.g., IP range) to block. |\n",
        "| **Model improvement** | Spot spurious correlations driving false positives. |\n",
        "| **Regulatory** | GDPR “right to explanation” for automated decisions. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "478187a0",
      "metadata": {
        "id": "478187a0"
      },
      "source": [
        "> **Colab Tip**: First run `!pip install shap -q` if SHAP is not pre‑installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b34d070",
      "metadata": {
        "id": "6b34d070"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd, shap, matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "import warnings, os\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.rcParams['figure.figsize'] = (6,4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f665289",
      "metadata": {
        "id": "0f665289"
      },
      "outputs": [],
      "source": [
        "# Create synthetic 'login attempt' dataset\n",
        "np.random.seed(42)\n",
        "n_normal = 500\n",
        "n_anom = 25\n",
        "\n",
        "normal = pd.DataFrame({\n",
        "    \"failed_logins_last_5min\": np.random.poisson(2, n_normal),\n",
        "    \"hour_of_day\": np.random.randint(0, 24, n_normal),\n",
        "    \"geo_distance_km\": np.random.gamma(2, 20, n_normal)  # distance from usual location\n",
        "})\n",
        "\n",
        "anom = pd.DataFrame({\n",
        "    \"failed_logins_last_5min\": np.random.poisson(15, n_anom),\n",
        "    \"hour_of_day\": np.random.randint(0, 24, n_anom),\n",
        "    \"geo_distance_km\": np.random.gamma(2, 50, n_anom) + 200\n",
        "})\n",
        "\n",
        "df = pd.concat([normal, anom], ignore_index=True)\n",
        "y = pd.Series([0]*n_normal + [1]*n_anom)  # 1 = anomaly\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e473864",
      "metadata": {
        "id": "7e473864"
      },
      "source": [
        "## 2️⃣ SHAP Basics\n",
        "\n",
        "**SHapley Additive exPlanations** decompose a prediction into feature contributions respecting game‑theoretic fairness.\n",
        "\n",
        "Visuals:\n",
        "* **Bar plot** – mean absolute SHAP value per feature (global importance).  \n",
        "* **Force plot** – per‑instance explanation (pushes ↑ prediction).\n",
        "\n",
        "> SHAP value sign indicates direction: positive value pushes *towards anomaly* (higher score here).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a88cd99",
      "metadata": {
        "id": "6a88cd99"
      },
      "source": [
        "### 2a. SHAP for Isolation Forest\n",
        "\n",
        "Tree‑based models use **`shap.TreeExplainer`** – fast, exact path‑based calculation.\n",
        "\n",
        "Mechanism  \n",
        "1. Compute expected model output.  \n",
        "2. Evaluate contribution of each feature to deviation from expectation.\n",
        "\n",
        "Interpretation: High positive SHAP for `failed_logins_last_5min` means that spike drove the anomaly score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790846a0",
      "metadata": {
        "id": "790846a0"
      },
      "outputs": [],
      "source": [
        "# Train Isolation Forest\n",
        "iso = IsolationForest(n_estimators=200, contamination=0.05, random_state=0)\n",
        "iso.fit(X_scaled)\n",
        "\n",
        "# Use TreeExplainer\n",
        "explainer_if = shap.TreeExplainer(iso)\n",
        "shap_values_if = explainer_if.shap_values(X_scaled)\n",
        "\n",
        "# Global bar plot\n",
        "shap.summary_plot(shap_values_if, df, plot_type=\"bar\", show=False)\n",
        "plt.title(\"IF – Global feature importance\")\n",
        "\n",
        "# Explain a single suspicious record (index -1, an anomaly)\n",
        "idx = len(df)-1\n",
        "print(\"Instance:\", df.iloc[idx].to_dict())\n",
        "shap.force_plot(explainer_if.expected_value, shap_values_if[idx], df.iloc[idx], matplotlib=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51db3f92",
      "metadata": {
        "id": "51db3f92"
      },
      "source": [
        "### Interpretation Guide – SHAP for Isolation Forest\n",
        "\n",
        "**Global bar plot**\n",
        "\n",
        "* **Ordering** – Longer bars = higher *mean absolute* impact on anomaly score.\n",
        "* **Units** – Same scale as the raw IF `decision_function`; positive value means *toward normal*, negative means *toward anomaly*.\n",
        "* **Take‑away** – Top‑ranked feature(s) are the primary drivers of anomalies across the whole dataset.\n",
        "\n",
        "**Per‑instance force plot**\n",
        "\n",
        "* **Grey base value** – Average IF score on the background sample (≈ “typical normal point”).\n",
        "* **Red arrows** – Features *lowering* the score below the threshold ⇒ make the point look anomalous.\n",
        "* **Blue arrows** – Features pushing the score up toward normal.\n",
        "* **Additivity check** – Sum of all arrows + base value = final model score (Shapley property).\n",
        "\n",
        "> **Insight for analysts**: Large red arrow on `failed_logins_last_5min` pinpoints excessive login failures as the actionable root‑cause of the alert.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52081ffe",
      "metadata": {
        "id": "52081ffe"
      },
      "source": [
        "### 2b. SHAP for Autoencoder\n",
        "\n",
        "Autoencoders aren’t tree‑based; we approximate explanations with **`KernelExplainer`** on the *reconstruction error* model.\n",
        "\n",
        "Steps  \n",
        "1. Wrap the AE to output reconstruction error per sample.  \n",
        "2. Use a sample of background data (normal points) for Shapley baseline.  \n",
        "3. Interpret SHAP values: features contributing most to *high* error caused the anomaly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fea75f8",
      "metadata": {
        "id": "0fea75f8"
      },
      "outputs": [],
      "source": [
        "# Train tiny autoencoder on normal only\n",
        "X_norm = X_scaled[y == 0]\n",
        "inp = layers.Input(shape=(X_norm.shape[1],))\n",
        "encoded = layers.Dense(10, activation='relu')(inp)\n",
        "decoded = layers.Dense(X_norm.shape[1], activation='linear')(encoded)\n",
        "ae = models.Model(inp, decoded)\n",
        "ae.compile(optimizer='adam', loss='mse')\n",
        "ae.fit(X_norm, X_norm, epochs=15, batch_size=64, verbose=0)\n",
        "\n",
        "# Wrapper: output MSE scalar\n",
        "import tensorflow as tf\n",
        "def mse_model(x):\n",
        "    recon = ae.predict(x, verbose=0)\n",
        "    return np.mean(np.square(recon - x), axis=1)\n",
        "\n",
        "# KernelExplainer (may be slow; use 100 background samples)\n",
        "background = X_norm[np.random.choice(len(X_norm), 100, replace=False)]\n",
        "explainer_ae = shap.KernelExplainer(mse_model, background)\n",
        "# Explain one anomaly instance\n",
        "sample = X_scaled[-1:].copy()\n",
        "shap_values_ae = explainer_ae.shap_values(sample, nsamples=200)\n",
        "\n",
        "# Visualise\n",
        "shap.force_plot(explainer_ae.expected_value, shap_values_ae[0], df.iloc[[-1]], matplotlib=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a7971ef",
      "metadata": {
        "id": "2a7971ef"
      },
      "source": [
        "### Interpretation Guide – SHAP for Autoencoder\n",
        "\n",
        "**Why different?**  We explain the **reconstruction MSE** (higher ⇒ more anomalous).\n",
        "\n",
        "* **Red arrows** – Features **increasing** MSE (push toward anomaly).\n",
        "* **Blue arrows** – Features decreasing MSE.\n",
        "\n",
        "**Caveats**\n",
        "\n",
        "1. **Approximate** – KernelExplainer samples the feature space; small variations are expected.\n",
        "2. **Scaling matters** – Ensure inputs are standardised; otherwise large‑scale features dominate SHAP magnitude.\n",
        "3. **High‑dimensional data** – SHAP values may become diffuse. Consider PCA or local masking to focus on top‑k features.\n",
        "\n",
        "> **Practical reading**: A dominant red arrow on `geo_distance_km` reveals the AE had never seen such a distance during training, hence cannot reconstruct it – that deviation generates high error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df5aa2ac",
      "metadata": {
        "id": "df5aa2ac"
      },
      "source": [
        "## 3️⃣ Knowledge Check\n",
        "\n",
        "1. **Trust & actionability** – How does a SHAP force plot help a security analyst decide next steps?  \n",
        "2. **TreeExplainer vs KernelExplainer** – When would you prefer KernelExplainer even for a tree model?  \n",
        "3. **Autoencoder explainability** – Why might SHAP values on raw reconstruction error be misleading in high‑dimensional data? Suggest a fix.  \n",
        "4. **Regulatory** – Outline a GDPR‑compliant explanation you could generate for an Isolation Forest’s fraud flag.  \n",
        "5. **Model improvement** – If SHAP reveals `hour_of_day` dominates anomaly scores but domain experts disagree, what steps would you take?\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
